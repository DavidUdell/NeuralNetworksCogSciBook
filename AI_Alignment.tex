\chapter{AI Alignment}\label{ch_ai_alignment}
\chapterauthor{David Udell}{1}

AI alignment is the field concerned with setting the goals of artificial
intelligences. The field pre-dates the deep learning revolution and modern
LLMs; it is especially interesting that (1) much of the modern post-training
pipeline for frontier LLMs grew out of this originally speculative field, and
(2) AI alignment as a field has, conversely, adapted itself to the advent of
existing capable AI in the form of our modern LLMs.

\section{AI Alignment Before the Deep Learning Revolution}
Before the advent of LLMs, AI in the form of symbolic systems seemed much more
relatively promising. Theoretical work about AI alignment began in this period,
but was in effect hedging its bets with regards to which paradigm of AI would
ultimately take off.

A lot can still be said about the goals of AIs and how they align with, or fail
to align with, the goals of humans \cite{bostrom2014superintelligence}. Much of
the theoretical machinery here was grounded in mathematical intuitions about
von Neumann-Morgenstern rationality.

Some important results dating from this period are a theoretical explication of
``corrigibility'', a formal model of inductive reasoning in terms of market
equilibria, and a formalization of the notion of ``limited'' optimization
pressure. Corrigibility is a potentally desirable property in an AI; it is the
idea that an AI should defer to its human overseer. Rather than just implanting
some set of goals in an AI and letting it run, corrigibility aims at having an
AI not learn any particular set of goals but instead come to be a generally
helpful assistant lacking its own goals.

\section{Base Models and Chat Models}
% GPT-2 does not have a complete personality. It's like auto-complete on
% steroids. They will insult you and do other unusual things emulating stuff on
% the internet. Some examples of this would be great, if you could recreate
% them, or even find examples. I like the idea of making really clear what the
% “raw” personality of these is. It’s a world model, but a giant conflagration
% of personalties too. Nothing at all consistent.

\subsection{Model Post-Training}
% So a new thing was needed, ''personality engendering'' (I guess turn-taking
% and length of responses also came out of this, right?) The helpful assistant
% persona we are used to didn’t just happen automatically. That took work. Also
% we will probably want to say something somewhere about... system prompts.

% \subsection{Supervised Chat Fine-Tuning}
% Much of the work done through this. ``The Void''/nostalgebraist

% \subsection{Reinforcement Learning from Human Feedback}
% Interesting that it's RL on the same SSL weights.

% \subsection{Reinforcement Learning from AI Feedback}
% Anthropic's
% Digression on Claude's personality/Amanda Askell

% \subsection{Reinforcement Learning from Verifiable Feedback}
% Chain of Thought and the Reasoning Revolution
% ''Let's think step by step'' paper
% Deepseek R1: polyglot reasoning in v1, ironed out in v2

\section{Prosaic AI Alignment}
% Digression on Sydney, ``MechaHitler'' Grok as failed examples. Claude's
% snitching, Claude's misalignment in the wild.

\section{Superintelligence Alignment}
% Superalignment at OpenAI, Anthropic
% AI Alignment teams

\subsection{AI Control}
% Elicit, HCH
% IDA
% Redwood Research
% AI Control teams

\subsection{Superintelligence and the Future}
% A new apex predator on Earth? Misalignment risk
% Deep Utopia, CEV
% OpenAI's claims--lamplighter essay
% Dario's essay
