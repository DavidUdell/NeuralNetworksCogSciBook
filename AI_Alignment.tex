\chapter{AI Alignment}\label{ch_ai_alignment}
\chapterauthor{David Udell}{1}

AI alignment is the field concerned with setting the goals of artificial
intelligences. The field pre-dates the deep learning revolution and modern
LLMs; it is especially interesting (1) that the modern post-training pipeline
for frontier LLMs grew out of ideas from this originally speculative field, and
(2) that AI alignment as a field has, conversely, had to adapt itself to the
advent of capable AI in the form of modern LLM paradigm. A theme in this
chapter is that intertwinement between the work of those who sought to set the
goals of powerful AI--whenever it came about--and the research happening at the
frontier AI labs--where powerful AI did come about. There's a story here about
the way that empirical technological reality butted up against pre-existing
anxieties about smarter-than-human AI, and about the sociotechnical equilibrium
that came out of that collision.

Our survey of AI alignment is roughly chronological, looking at the state of
the field before deep learning, today, and forward into the future.

\section{AI Alignment, Before the Deep Learning Revolution}
AI as a research field is quite old now, dating back to 1956. AlexNet (2012)
\cite{krizhevsky2012imagenet} and the transformer (2017)
\cite{vaswani2017attention}, and the whole contemporary explosion of AI
capabilities and development, came about much later. It was between these two
bookends that AI alignment was born. The field is, in great part, the
brainchild of Eliezer Yudkowsky, a decision theorist and founder of the Machine
Intelligence Research Institute \cite{yudkowsky2008factor}.

A lot could still be said about the possible goals of AIs and how they might or
might not line up with the goals of their human creators
\cite{bostrom2014superintelligence,omohundro2008drives,yudkowsky2008factor}.
Without concrete powerful AI on hand, many potential future paradigms had to
be, in effect, speculated about and averaged over. Theoretical work at the time
grounded itself in mathematical intuitions about von Neumann-Morgenstern
rationality \cite{von1944games}, leaving open how AI capabilities would
actually be filled in \cite{soares2015corrigibility}.

A formalization of AI ``corrigibility'' is one of the AI alignment results
tracing back to this period \cite{soares2015corrigibility}. Informally,
corrigibility is the idea that an AI should not learn any particular set of
goals but instead come to be a generally helpful assistant lacking its own
goals. More precisely, corrigibility could be thought about in terms of an AI
letting a human shut it down at any point, without actively trying to prevent
itself from being shut down. It turns out that it's quite difficult to give a
formal model of an agent that wants to help but never ``takes the reins''.

EXPLICATE THIS

One important paper gave a formal model of inductive (non-deductive) reasoning
in terms of market equilibria between idealized bettors
\cite{garrabrant2020induction}. Another formalized the intuitive notion of
``limited'' optimization \cite{taylor2016quantilizers}. Research in this vein
was termed ``agent foundations'': essentially, an approach to GOFAI that sought
to understand AI from the ground up, with the goal of thereby also
understanding and setting the AI's goals.

\section{Base Models and Chat Models}
% GPT-2 does not have a complete personality. It's like auto-complete on
% steroids. They will insult you and do other unusual things emulating stuff on
% the internet. Some examples of this would be great, if you could recreate
% them, or even find examples. I like the idea of making really clear what the
% “raw” personality of these is. It’s a world model, but a giant conflagration
% of personalties too. Nothing at all consistent.

\subsection{Model Post-Training}
% So a new thing was needed, ''personality engendering'' (I guess turn-taking
% and length of responses also came out of this, right?) The helpful assistant
% persona we are used to didn’t just happen automatically. That took work. Also
% we will probably want to say something somewhere about... system prompts.

% \subsection{Supervised Chat Fine-Tuning}
% Much of the work done through this. ``The Void''/nostalgebraist
\cite{askell2021assistant}
\cite{nostalgebraist2025void}

% \subsection{Reinforcement Learning from Human Feedback}
\cite{ziegler2020human,ouyang2022feedback}
% Interesting that it's RL on the same SSL weights.

% \subsection{Reinforcement Learning from AI Feedback}
% Anthropic's
\cite{bai2022training}
\cite{bai2022constitutional}
% Digression on Claude's personality/Amanda Askell
\cite{sharma2025sycophancy}
\cite{backlund2025vending}


% \subsection{Reinforcement Learning from Verifiable Rewards}
\cite{lambert2025tulu}
% Chain of Thought and the Reasoning Revolution
% ''Let's think step by step'' paper
\cite{kojima2023zeroshot}
% Deepseek R1: language mixing in v1's reasoning, ironed out in v2
\cite{deepseekai2025deepseek}
\cite{silver2025experience}

\section{Prosaic AI Alignment}
\cite{krakovna2020gaming,metr2025hacking}
% Digression on Sydney, ``MechaHitler'' Grok as failed examples. Claude's
% snitching, Claude's misalignment in the wild.
\cite{greenblatt2024faking}
\cite{hubinger2024sleeper}
\cite{lynch2025agentic}
% Emergent misalignment and "who am I"
\cite{betley2025emergent}

\section{Superintelligence Alignment}
\cite{leike2023superalignment}
% Superalignment at OpenAI, Anthropic
\cite{anthropic2025rsp}
% AI Alignment teams

\subsection{AI Control}
% Elicit, HCH
% IDA
% Redwood Research
% AI Control teams
\cite{greenblatt2024aicontrol}

% Models' situational awareness
\cite{derner2024truesight,needham2025know}

% CoT monitoring; faithfulness
\cite{baker2025monitoring}

\subsection{Superintelligence and the Future}
\cite{aschenbrenner2024situational}
% A new apex predator on Earth? Misalignment risk
% Deep Utopia, CEV
\cite{bostrom2024utopia}
% OpenAI's claims--lamplighter essay
\cite{altman2024intelligence}
% Dario's essay
\cite{amodei2024grace}
