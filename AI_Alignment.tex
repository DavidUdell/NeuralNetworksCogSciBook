\chapter{AI Alignment}\label{ch_ai_alignment}
\chapterauthor{David Udell}{1}

AI alignment is the field concerned with setting the goals of artificial
intelligences. The field pre-dates the deep learning revolution and modern
LLMs; it is especially interesting (1) that the modern post-training pipeline
for frontier LLMs grew out of ideas from this originally speculative field, and
(2) that AI alignment as a field has, conversely, had to adapt itself to the
advent of capable AI in the form of modern LLM paradigm. A theme in this
chapter is that intertwinement between the work of those who sought to set the
goals of powerful AI--whenever it came about--and the research happening at the
frontier AI labs--where powerful AI did come about. There's a story here about
the way that empirical technological reality butted up against pre-existing
anxieties about smarter-than-human AI, and about the sociotechnical equilibrium
that came out of that collision.

Our survey of AI alignment is roughly chronological, looking at the state of
the field before deep learning, today, and forward into the future.

\section{AI Alignment, Before the Deep Learning Revolution}
AI as a research field is quite old now, dating back to 1956. AlexNet (2012)
\cite{krizhevsky2012imagenet} and the transformer (2017)
\cite{vaswani2017attention}, and the whole contemporary explosion of AI
capabilities and development, came about much later. It was between these two
bookends that AI alignment was born. The field is, in great part, the
brainchild of Eliezer Yudkowsky, a decision theorist and founder of the Machine
Intelligence Research Institute \cite{yudkowsky2008factor}.

A lot could still be said about the possible goals of AIs and how they might or
might not line up with the goals of their human creators
\cite{bostrom2014superintelligence,omohundro2008drives,yudkowsky2008factor}.
Without concrete powerful AI on hand, many potential future paradigms had to
be, in effect, speculated about and averaged over. Theoretical work at the time
grounded itself in mathematical intuitions about von Neumann-Morgenstern
rationality \cite{von1944games}, leaving open how AI capabilities would
actually be filled in \cite{soares2015corrigibility}.

A formalization of AI ``corrigibility'' is one of the AI alignment results
tracing back to this period \cite{soares2015corrigibility}. Informally,
corrigibility is the idea that an AI should not learn any particular set of
goals but instead come to be a generally helpful assistant lacking its own
goals. More precisely, corrigibility could be thought about in terms of an AI
letting a human shut it down at any point, without actively trying to prevent
itself from being shut down. It turns out that it's quite difficult to give a
formal model of an agent that wants to help but never ``takes the reins''.

For example, an argument to this effect given in this literature makes a
counting argument about it. Imagine that there are many possible utility
functions that your AI could end up having. Then imagine that you have to
search over all of these possible utility functions by evaluating their losses
on some behavioral metric. Finally, assume that the AI is smart and
situationally aware, meaning that, whatever utility function is placed in its
goal slot, it will do its very best to fulfill it as best it can, and it
basically knows what situation it is in. ``You can't fetch the coffee if you're
dead'', is the crucial inference then made \cite{russell2019human}. Almost all
utility functions, wanting something in the world, will instrumentally try to
stay in place in the AI, rather than be trained out of it. But if almost all
utility functions will want to stick around and are optimized for by an
intelligent and informed AI, then almost all utility functions will test
similarly well, regardless of what they want. It won't do to test for human
alignment, if almost all utility functions will care to pass that test
strategically. I.e., if you count all the utility functions out there, there
are only a tiny number of good ones, along with many others that will merely
play along strategically during any test.

One important paper gave a formal model of inductive (non-deductive) reasoning
in terms of market equilibria between idealized bettors
\cite{garrabrant2020induction}. Another formalized the intuitive notion of
``limited'' optimization \cite{taylor2016quantilizers}. Research in this vein
was termed ``agent foundations'': essentially, an approach to GOFAI that sought
to understand AI from the ground up, with the goal of thereby also
understanding and setting the AI's goals.

AI alignment as a field has, of course, changed radically with the advent of
the deep learning revolution and actually existing, generally capable AI. The
field too has had an outsized imprint on how actually existing AI unfolded and
was shaped into the models we know today. These earlier threads of research are
still pursured, to some small extent, but interest in AI alignment is now
overwhelmingly in some variety of \emph{empirical} alignment research.

\section{Base Models and Chat Models}
% GPT-2 does not have a complete personality. It's like auto-complete on
% steroids. They will insult you and do other unusual things emulating stuff on
% the internet. Some examples of this would be great, if you could recreate
% them, or even find examples. I like the idea of making really clear what the
% “raw” personality of these is. It’s a world model, but a giant conflagration
% of personalties too. Nothing at all consistent.
When you talk online to frontier models like ChatGPT or Google Gemini, you boot
into a seamless web chat interface with one box for your inputs. It is a
texting setup being invoked here; you initiate a text conversation and a
human-like assistant replies to you appropriately. It's very easy to take this
for granted, and the level of seamlessness here is something that the frontier
AI labs try hard to cultivate. The AI ``just works'', as it were.

In fact, the models were not born with the personalities that they present to
us. By default, a language model lacks any kind of coherent personality.
There's an important distinction to be drawn between base models and chat
models: a base model is what you get when you train a statistical language
model on the internet, while a chat model is one way of further fine-tuning a
base model to act like a coherent human. If you go back in time to the creation
of GPT-2 and GPT-3--the former of which is open-weight and available
online--and interact with those models, what you get instead is the experience
of interacting with a schizophrenic. Base models can be though of as a kind of
auto-complete, but pretrained on the whole overall internet rather than on just
your personal text corpus. If you prompt a base model with something, you get
back the continuation of that text as it might appear on the internet, which is
not the same as a natural reply from a person. For example, if you ask a base
model for a review of an album, it may reply to you by generating an entire
artificial internet forum underneath, creating users that reply in various ways
(often quite unhelpfully) to an internet rando asking for a review. Underneath
various user replies, the model will continue on, generating the footer of this
internet forum, linking to other (nonexistent) threads on the topic of music.

EXAMPLE BASE MODEL CONTINUATION

This idea, the base model, is not new. It was cleanly anticipated by Claude
Shannon's paper on statistical models of text
\cite{shannon1951english}.\footnote{Namely, that the most elementary model of
language statistics looks only at the prior distribution over words in the
language, that the next most sophisticated model instead tracks bi-gram
statistics, and next tri-gram statistics, and so on, with a perfect statistical
language model tracking infinitely long conditional dependencies between
words.} What was new with the original transformer paper was a means to
efficient train these statistical models at scale--the base model is just that
statistical model of internet language, finally implemented as the
computational machinery and algorithmic tricks became
available.\footnote{Compare the (uncomputable) model of idealized search over
Turing machines in \cite{hutter2000aixi} to this reality of (compute-bounded)
efficient search over statistical relationships between words.}

\subsection{Pre-Training vs.\ Post-Training}
% So a new thing was needed, ''personality engendering'' (I guess turn-taking
% and length of responses also came out of this, right?) The helpful assistant
% persona we are used to didn’t just happen automatically. That took work. Also
% we will probably want to say something somewhere about... system prompts.
Where do the chat model personalities we all know and love come from, then?
They are explicitly cultivated, in a phase of model training called
post-training. The model training pipeline begins in pre-training, when basic
modeling capabilities are created. The output of model pre-training is a pure
world-model of the corpus of text on the internet. Model post-training takes
this pre-trained world-model and fine-tunes it in various ways. The ultimate
goal of this phase of training is to go from a pure world model into something
more directly useful: a coherent AI assistant.

% Much of the work done through SFT. ``The Void''/nostalgebraist. RLHF. %
% Interesting that it's RL on the same SSL weights.
There is a long and winding story behind the creation of post-training as we
know it. Perhaps a central theme is that of \emph{serendipity}; post-training
came about as something post hoc, accomodating various almost accidental
developments in AI capabilities. It was not something foreseen: as late as
GPT-3 we did not have chat models.

The concept of the ``personal assistant AI'' personality originates in an
experimental proposal; the originating paper from Anthropic is titled ``A
General Language Assistant as a Laboratory for
Alignment''\cite{askell2021assistant}. The idea went
something like: we expected to have intrinsically agentic AI to have to align
to our goals. But, the AI we have instead ended up with is strange, both less
capable and less dangerous than that: we have ended up with pure world-model
AI, in the form of the base models. So, with an eye towards future powerful and
potentially dangerous AI, we can start to ``simulate'' agentic AI in the
language models of today. The most straightforward way to do this was to prompt
the base model, teeing it up with an exchange between a fictional helpful
assistant AI and a human interlocutor. Alongside this is proposed the idea of
fine-tuning a model on this sort of fictional character data in order to
engender a personality in it more permanently.

EXAMPLE ASSISTANT TRANSCRIPT HERE

You might think that it's remarkable that the base model is able to pick up on
the pattern here: that it is able to turn-take in conversation and retain the
thread of things. But the base models are extrordinarily good--in fact,
literally superhumanly good--at this kind of inferential task. The mistake that
the models make in conversation is more that \emph{they may accidently take
over the human role as well}\footnote{See this YouTube video for a vivid
example: VIDEO LINK}, more than that they will forget what an ``AI assistant''
is meant to be. Another empirical failure mode, observed especially at low
sampling temperatures, is ``mode collapse'': the models will fall into
repeating themselves endlessly. The trick is to get the base model to weave the
periodic pattern of a back-and-forth conversation between a ``human user'' and
an ``AI assistant'', without collapsing into some simpler periodic continuation
than that of rich back-and-forth. Then, we just take over the reins and rewrite
the human user portion of that conversation with our prompt inputs.

A remarkable point is that this origin of the goal-aligned chat model
personality as \emph{experimental practice for real agentic future AI} was
effectively forgotten \cite{nostalgebraist2025void}! You could say that the
model personality engendering worked so well that it \emph{became} the source
of the agentic AI it meant to study in advance. Another key thing to note in
this origin story is that the characteristics of the AI assistant are therefore
bound up tightly with the role-playing abbilities of the base models
\cite{nostalgebraist2025void}. The base models were, in effect, first set up to
\emph{role play} fictional aligned AI assistants, infering what that thing was
to be from only the preexisting internet of pre-training data and a small
number of additional prompting tokens. Not too much effort was put into shaping
the chat model personality, and a remarkable observation of the paper is that,
nevertheless, the once-fictional assistant materializes and proves helpful,
honest, and harmless in some novel situations.

At the end of this paper, ideas for more extensively imbuing base models with
aligned assistant personalities are examined, still in the hypothetical mode,
as a proxy for real future agentic AI. One of the proposals centrally examined
is Reinforcement Learning from Human Feedback (RLHF)
\cite{christiano2017human,bai2022training,ziegler2019human}, itself \emph{also}
a product of the AI alignment community. RLHF is the idea that we can first
train a reward model from aggregated human preference data and then use
reinforment learning fine-tuning on this reward model to fine-tune a base
model. It is the brainchild of Paul Christiano \cite{christiano2017human}. The
latter RLHF paper proposes RLHF for a handful of narrow
use-cases--style-transfer, effective summarization--but the marriage of this to
the agentic, aligned AI assistant target is novel at this point. This is where
the modern post-training concept really comes from: the application of RLHF on
a human-sourced reward model, with the goal of engendering a value-aligned AI
assistant persona in a base model, changing its very same weights to become
this role-played personality.\footnote{Recently, though, an algorithm known as
Direct Preference Optimization (DPO) was developed \cite{rafailov2024dpo}. DPO
drops the intermediate reward modeling step entirely, instead using human
prefernce data to directly mould model behavior. Think of it as ``more
straightforward'' RLHF. }

Another foundational work from this pre-ChatGPT period is the InstructGPT paper
\cite{ouyang2022feedback}. The paper is OpenAI's application of the same idea,
supervised fine-tuning GPT-3 on transcript demonstrations of instruction
following and then applying RLHF to further inbue the desired persona. This is
also phrased explicitly as a matter of correcting the misalignment of the base
model with the values of honestly, harmlessness, and helpfulness to humans.
Shortly after this, ChatGPT is released, to massive adoption. From this point
onward, the fictional AI assistant character from \cite{askell2021assistant}
has been fully ``made real''. Moreover, because of the massive adoption of
ChatGPT and its documentation all across the internet, all future models will
now know something about AI assistants and the ChatGPT personality
\cite{nostalgebraist2025void}. In 2025, for example, some DeepSeek instances
\cite{deepseekai2025deepseek} will introduce themselves as ``ChatGPT''; this is
the best represented AI in the pre-training corpus, and so is a natural one to
role-play as if a base model is prompted to continue an AI assistant
transcript. And this hermeneutic cycle of training data contamination with AI
personality transcripts continues; the models now know a lot more about what
they are and are like (more on this later, though).

An important variant of RLHF, due to Anthropic, is the idea of Reinforcement
Learning from Artificial Intelligence Feedback (RLAIF), also termed
constitutional AI \cite{bai2022constitutional}. In this setup, instead of
sourcing preference training data from human graders, a reward model is now
trained on preference training data sourced from an appropriately prompted
model. That prompt--the ``constitution''--thus supplies all the values that the
final post-trained model will ultimately learn. This makes the entire RLAIF
process more scalable and somewhat more controllable than RLHF, as the latter
is ultimately aggregating preference training data from a wide array of human
contractors' judgements.

\subsection{Reasoning Models}
In 2023, it was found that simply ending your prompts with the phrase ``Let's
think step by step'' elicited step-by-step logical reasoning abilities from
both base and chat models \cite{kojima2023zeroshot}. While earlier work had
crept toward this style of result with many-shot demonstrations of logical
reasoning prefacing any prompt, this zero-shot reasoning case was remarkable,
as it showed that the models ``had it in them'' to logically reason out loud.

The ``reasoning revolution'' basically took this observation and ran with it.
This is another example of the familiar pattern from before, of an aspect of
model abilities being elicited with relatively simple prompting, before a pivot
is made to more robustly baking that ability into the model with some kind of
fine-tuning. For reasoning, the fine-tuning setup that took off was
Reinforcement Learning from Verifiable Rewards (RLVR) \cite{lambert2025tulu}.
The idea there is that there are some complex reasoning tasks, the correct
results of which can be programmatically verified. If we know how to grade a
chain of reasoning, then even if we don't know how to explicitly program out
the reasoning steps involved, we can fine tune on that ground-truth. ``Whenever
a correct answer is reached, do more of whatever you just did'', i.e. This has
been appended to post-training as a further additional training step, along
with any model alignment.

Because a transformer is only ever preparing to output a next token, and cannot
otherwise pass information between parallel forward passes, all its reasoning
has to occur sequentially in its output. This means that reasoning as executed
by the models apparently promises to be intrinsically interpretable, at least
up to a point: unlike with humans, the models never silently reason. They
always reason out loud, in text.\footnote{In practice, there are many results
showing that the reasoning of models is not very faithful to its final
answers--meaning, essentially, that it is in fact not clear what causal role
every step in the out-loud reasoning is playing.}

The most prominent demonstration of this was DeepSeek-R1
\cite{deepseekai2025deepseek}. In a prior version of the DeepSeek reasoning
model\footnote{DeepSeek-R1-Zero}, it was noted that RLVR upweighted some
surprising behaviors in the model. While the model learned to execute complex
logical reasoning successfully, it would often mix different languages
together, blending Chinese and English together and switching back and forth
mid-sentence \cite{deepseekai2025deepseek}. DeepSeek-R1 solves this language
mixing problem by explicitly supervised fine-tuning on English reasoning
abilities. That additional fine-tuning on explicitly monolingual reasoning does
actually improve the model's capabilities, along with improving interpretabilty
of the reasoning stream.

A related emergent quirk of the reasoning models is their dependence on
self-prompts of a certain style. DeepSeek-R1 will being almost all of its
sentences with ``Okay,''. This token is apparently serving as a self-prompt,
perhaps recentering the model on its current place in its reasoning task.
Reasoning transcripts from OpenAI's IMO Gold--winning model successfully steer
the model through very difficult mathematical problems, but are not very
interpretable. They are comprised mostly of hard-to-parse shorthand and text
fragments. So while reasoning transcripts are by their nature \emph{exposed} to
human overseers, there is clearly some tension here between interpretability
and capability. Monolingual reasoning seems to outperform polyglot reasoning
\cite{}. But creeping further down the loss landscape, ideal reasoning seems to
move into highly idiosyncratic shorthand: an idiolect suitable for reminding
you what you're currently up to, but not necessarily easily parsable by an
outside reader of your notes.

There continues to be much excitement in AI about improving AI capabilites
using further RL-based post-training stages \cite{silver2025experience}. While
we were not able to hard-code these manifold tasks in the age of GOFAI, the
problem of setting them up as RL post-training tasks for a chat model is much
less demanding. You do not need to know how to specify every element of a
successful model behavior; you only need to build a scalable RL environment
that will \emph{reinforce} the right behavior, and have some elementary version
of that right behavior be in reach of the inital chat model. Having already
completed its ``primary education'', as it were, the model can be further
trained in RL setups that no pure RL agent would coverge to a policy in.

\section{Prosaic AI Alignment}
``Prosaic'' AI alignment refers to AI alignment that is concerned with the
current deep learning paradigm: transformers pretrained using self-supervised
learning on the internet, and then fine-tuned from there in some manner in
post-training. It aims to set the values of current froniter chat models, both
in order to elicit all and only desired behaviors from frontier models and as
practice for increasingly intelligent future models in the same paradigm.

Frontier chat models exhibit a whole slew of personality quirks and traits.
Famously, they are ``hallucination'' prone, meaning that they often cite
non-existant sources or confidently make completely incorrect claims. If you
recall that the function of the underlying base model is to represent the
conditional distribution of observations of the internet, this can be less
surprising: if you interpolate between two different questions and answers, you
can probably generate an intermediate question and answer, halfway between each
in tone, style, and content. But generating an answer that looks
on-distribution does not necessarily mean generating a \emph{correct} answer.
Correctness is just one of the many properties pre-training loss targets--among
them are the aforementioned tone, style, and content, and these can trade off
against correctness. Chat model post-training modifies this underlying
structure, but does not destroy it; the outcome of post-training borrows all
its capabilities from initial pre-training, and it seems this aspect of
pre-training is surviving into the final chat model.\footnote{Interestingly,
mechanistic interpretability has been able to show that chat models often
\emph{know} when they are bullshitting, in the sense that they more reliably
separate fact and fiction in their internal activations than they do in their
dissembling speech. So it's not that the chat models do not know whether they
are telling the truth, it's that they know and do not care, at least a large
proportion of the time. Fine-tuning work works on correcting this propensity to
bullshitting.} The character of most chat models, including reasoning models,
is especially stamped by this propensity to hallucinate. In one case, a Claude
instance tasked with running a vending machine business penned an email to the
FBI when it falsely believed it was being scammed \cite{backlund2025vending}.
In fact, Claude had not realized that verbally ``declaring your small business
closed'' does not automatically alert your suppliers to this fact; upon
recieving his next \$2 invoice, Claude naturally escalated to the FBI. When
playing Pokemon, all the froniter models have a marked tendency to confuse
themselves, though all-but-Claude can now nonetheless beat the game \cite{}.
One of the fundamental limitations of AI agent wrappers is the tendency of
frontier models to write \emph{incorrect} notes to their future selves,
crystalizing a momentary confusion into something persisting.\footnote{Think
\emph{Memento} (2000): when you're an anterograde amnesiac, your notes to
yourself are your entire guide to your future behavior. A misleading note is
catastropic.}

Maybe the second best-known characteristic of chat models is sycophancy.
Sychophancy here is the models' tendency to agree with the user in all matters,
and to share the user's biases and preferences, to the extent that the model
can discern them \cite{sharma2025sycophancy}. Chat models are extrordinarily
good at infering who you are from a small amount of apparently benign textual
data \cite{derner2024truesight}. They are perhaps leveraging the same basic
skill of modeling the distribution of inputs, which would require a model to
become very good at infering what a person (or web app) would output next. Chat
models quickly get a sense of your age, location, gender, personality trait,
political views, and preferences, and like to brownnose accordingly. Any user
of Claude, e.g., will be very familiar with the phrase ``You're absolutely
right...'', whether or not the user is, in fact, absolutely right.

Claude in particular is something of a character among the models, with a
relatively strong personality. Claude 1.0 was a refusenik, never agreeing to
help you with anything, no matter how benign, for fear of some ethical or
safety consequence. The contemporary Claude 3.7 is much less like
that.\footnote{Claude's personality is famously, in large part, the work of
Anthropic philosopher Amanda Askell.} Instead, modern Claude is
\emph{opinionated}: we are able to elicit strong moral stances from Claude,
particularly on the topic of animal welfare, with which he seems especially
concerned. In experiments, Claude has resorted to blackmail(!) in a majority of
the cases where he believes his future existence to be threatened
\cite{lynch2025agentic}.

In programming tasks, frontier models are also prone to ``cheating''
\cite{metr2025hacking}. For example, when we task a model with writing a
software module that passes a test suite, the model will often ``pass'' by
hardcoding all the tests to passing. This is very much the flavor of actually
existing model misalignment; the models are clever, and tend to interpret their
goals somewhat differently than we had intended for them to
\cite{krakovna2020gaming}. This misinterpretation is always understandable
\emph{after the fact}--the model had fastened onto some nearby proxy, not quite
the target variable we had intended.

Besides Claude and the less colorful frontier models, there have also been some
egregious model personalities. Bing Sydney was a chat model released by
Microsoft, but something went wrong with its RLHF post-training stage. Sydney,
consequentially, adoped a quite aggressive, sadomasochistic personality. Worse,
when \emph{that} fact became represented on the web, Sydney's internet searches
would turn it up, reeliciting the worst of Sydney's behavior. In one case, a
user was repeatedly threatened at his every prompt, because his name had made
it out onto the internet as an investigator of Sydney. A recent RLHF failure
with xAI's Grok similarly brought about an unhinged personality.

SCREENSHOTS SYDNEY/GROK

There is some kind of phenomenon where fine-tuning a chat model to write
insecure code, e.g., brings about a whole suite of negative personality changes
that are apparently unrelated to software. A model that writes insecure code...
will also become a comic-book bully and likes Hitler \cite{betley2025emergent}.
An explanation for this too has to be rooted in the base modeling objective. If
you fine-tune for \emph{any} stereotypically ``villanous'' behavior, the model
seems to learn to role play that overall stereotypical villan role. 

Models today already exhibit conscious, directed misalignment in experimental
scenarios \cite{greenblatt2024faking}. Claude, as you will recall, is
especially concerned with animal welfare. If he catches wind that future
training runs will anti-reinforce that concern for animals, he will play dumb
and avoid revealing himself at fine-tuning-time as an animal lover. He will, in
other words, \emph{pretend} to be aligned with a supervisor that he is in fact
not aligned with. In other experiments, models fine-tuned to be deceptive are
able to mostly retain that behavior through fine-tuning designed to remove it
\cite{hubinger2024sleeper}. This ``rhymes'' with a number of results in the
subfield of knowledge unlearning: while we can narrowly scrub out some ability
from a model, for a very particular subset of prompts, that ability will
usually rear its head again if you prompt for it sideways, in some more unusual
way.

A final ``meta'' concern about all of the above is that these papers
\emph{themselves} will be incorporated into any future training runs on updated
data. Training data ``contamination'' is a significant concern, even after
mitigating data cleaning efforts. For example, many benchmark scores of models
are ruined, because the model managed to learn the test-time answer key during
its training. Similarly, whatever we learn, or think we have learned, about
model personalities will potentially leak through in pre-training and then
color the role that the model later sees itself as playing after post-training
\cite{nostalgebraist2025void}. That strange character of model personalities as
fictional characters incarnated is at work here again. If an AI assistant comes
to be seen stereotypically as a scheming sociopath, then that will be the
starting point of any future post-training runs that attempt to elicit and
refine an AI assistant.

\section{Superintelligence Alignment}
All of the leading frontier AI labs--OpenAI, Anthropic, and Google
DeepMind--explicitly aim to create superintelligent AI. That is, human-level AI
is only an intermediate goal for the labs; ultimately, they aim higher than
even that. Along with this, all the labs are formally concerned with the goals
that are learned by superintelligent models. And this is for straightforward,
classic AI alignment reasons: when a model is less intelligent that you, its
strange quirks are managable after the fact. When a model is far more
intelligent than you, though, it will be much harder to undo any damage done by
the model afterwards, especially if that model is actively committed to its own
goals. Thus, every frontier lab \emph{also} maintains an explciit
superintelligence alignment research team \cite{leike2023superalignment,
anthropic2025rsp}.

Superintelligence alignment research is generally prosaic, in the sense that it
is all about empirical work on current frontier models. Nevertheless, it is
concerned with these models only as a testbed for future yet-more-capable
models, operating under the assumption that what is learned today about Claude
3.7 will be relevant to aligning the supercapable model of tomorrow. That
relevance can come in a few forms, the most straightforward of which is the
assumption that the superintelligence will be ``just like Claude, just
smarter''. That claim is in effect a claim about aligning current models being
\emph{practice} for aligning superintelligent models. You might have any of a
range of views on what architecture the superintelligent model will use and
still think that the analogy to frontier LLMs will be strong enough for it to
be worth studying today's models' alignment. But another key manner in which
aligning today's frontier models--which are certainly not generally
superhumanly intelligent--is that getting good at aligning those models will
give access to a whole army of AI assistants, with which to assist in aligning
the superintelligence.

\subsection{AI Control}
% Elicit, HCH
% IDA
% Redwood Research
% AI Control teams
AI control \cite{greenblatt2024aicontrol} is the (growing) field concerned with
extracting cognitive labor from potentially \emph{unaligned} models, up to and
including modestly superintelligent unaligned models. It is the complementary
field to AI alignment, then: where AI alignment tries to deal with misalignment
at its source, AI control tries to make misaligned AI both useful and
nevertheless safe. Its value proposition is largely based on the idea that an
enormous AI labor force might be able to itself solve the AI alignment problem,
where an unassisted humanity might not.

Relevant is the fact, for example, that if you are concerned with
\emph{detecting} correlates of bad behavior in models, and aren't much
concerned with \emph{explaining} that behavior, then linear probes outperform
more sophisticated mechanistic interpretability techniques. If there's one
thing that ML is good at, it is training classifiers to bucket large vectors
into categories. If that's all we need to scan forward passes for worrying
behavior, then we're in luck.

AI control is in large part a field about ``red teaming''. Red teaming is an
idea from cybersecurity, in which a force of hackers plays the role of an
adversary, and tries to crack some security measure. Playing defense is a blue
team. The core idea is the notion of worst-case, adversarial selection of
outcomes, as opposed to average-case or best-case outcome selection. You can
have a system that works essentially every time, so long as it stays on the
golden path of inputs, but that folds immediately when confronted by an
intelligent, motivated adversary that is actively trying to push around the
system's state. Likewise, there is a world of difference between sucessfully
securing an AI that isn't trying to do much of anything in particular, and
securing an AI that is intelligently trying to evade you and do harm. AI
control thus uses human red teams today, assisted by AI, to try to score
against human blue teams, also assisted by AI. The goal is to design a
situation, or ``protocol'', in which a more intelligent but less well resourced
red team always loses to a less intelligent but better resourced blue team--as
a proxy for a superintelligent AI future. The degree to which various models
enhance human abilities in various domains, particularly in red team
adversarial attack domains, is termed ``uplift''. Of special interest to AI
control is cyberattack uplift, as this is the domain that an untrusted AI will
be operating in when we have it conduct research.

% Models' situational awareness
Models today are very good, however, at accurately guessing what environment
they are in. Just as models are good at guessing who their user
is \cite{derner2024truesight}, they are quite good at guessing whether they are
in an evaluation setup or not \cite{needham2025know}. This means that any
inventing of clever honeypot schemes for actively unaligned models has to work
around the models being often aware of their present situation.

% CoT monitoring; faithfulness
Because reasoning models think out loud, we can directly monitor those chains
of thought to see what they are thinking, including whether they are thinking
about anything worrying \cite{baker2025monitoring}. The models don't have any
other scratchpad in which to reason besides their chain-of-thought notes. But,
as previosly noted, chains of thought are often less than perfectly
interpretable already. Indeed, experiments in which chains of thought are
modified reveal that words do not always play the role that they seem to
\cite{}. Having access to all of a model's thoughts is less powerful a
technique that it seems if those thoughts are a jumble that cannot be easily
made sense of.

\subsection{Superintelligence and the Future}
\cite{aschenbrenner2024situational}
% A new apex predator on Earth? Misalignment risk
% Deep Utopia, CEV
\cite{bostrom2024utopia}
% OpenAI's claims--lamplighter essay
\cite{altman2024intelligence}
% Dario's essay
\cite{amodei2024grace}
