\chapter{AI Alignment}\label{ch_ai_alignment}
\chapterauthor{David Udell}{1}

AI alignment is the field concerned with setting the goals of artificial
intelligences. The field pre-dates the deep learning revolution and modern
LLMs; it is especially interesting (1) that the modern post-training pipeline
for frontier LLMs grew out of ideas from this originally speculative field, and
(2) how AI alignment as a field has, conversely, adaped itself to the arrival
of actually capable AI in the form of modern LLMs. A theme in this chapter is
that intertwinement between the work of those who sought to set the goals of
powerful AI--whenever it came about--and the research taking place at the
frontier AI labs--where powerful AI \emph{did} come about. There's a story here
about the way that empirical technological reality butted its head up against
pre-existing anxieties about smarter-than-human AI, and about the
sociotechnical equilibrium that fell out of that collision.

Our survey of field is very roughly chronological, looking at how the field
evolved from before deep learning up to today, and then finally at what the
future means to AI alignment.

\section{AI Alignment, Before the Deep Learning Revolution}
AI as a research field is surprisingly old now, formally dating back to the
1956 Dartmouth Conference. On the other hand, AlexNet (2012)
\cite{krizhevsky2012imagenet} and the transformer architecture (2017)
\cite{vaswani2017attention} and the whole ongoing explosion in AI capabilities
and interest in AI, came about much later. It is especially a post-2015
phenomenon. The field of AI alignment dates to somewhere inbetween those two
bookend dates. It is, in great part, the brainchild of Eliezer Yudkowsky, a
decision theorist and founder of the Machine Intelligence Research Institute
(MIRI) \cite{yudkowsky2008factor}.

As it predates modern LLMs, AI alignment in fact predates \emph{AI} as we now
know it. Nonetheless, a lot could still be said about the potential goals of AI
systems and about how AI goals might or might not line up with the goals of
their human creators
\cite{bostrom2014superintelligence,omohundro2008drives,yudkowsky2008factor}. AI
alignment research from this time had to be, in effect, speculate about and
prepare for possible AI futures, working in a vacuum. Theoretical work at the
time instead grounded itself in mathematical intuitions about von
Neumann-Morgenstern rationality \cite{von1944games} and standard decision
theory, not so much in statistical machine learning (the paradigm that we now
know succeeded), leaving open which form powerful AI capabilities would come
about in \cite{soares2015corrigibility}. This overall vein of research is
termed agent foundations. It is essentially AI alignment \emph{as married to
GOFAI and decision theory}. Work here always has the flavor of seeking to
understand AI completely, from the ground up, with the goal of understanding
the AI's goals as a consequence of... just completely understanding the AI's
structure.

\subsection{Agent Foundations}
To give some of the flavor of this research direction, the formalization of
``corrigibility'' is one of the key results tracing back to this period and
MIRI \cite{soares2015corrigibility}. Corrigibility is the idea of an AI not
learning \emph{any particular} goals but instead learning to be a generally
helpful AI assistant (entirely lacking its own goals).\footnote{Paul
Christiano, another leading figure in AI alignment, phrases this as the AI
sharing your goals \emph{de dicto}, rather than sharing your goals \emph{de
re}: sharing your goals \emph{whatever they might be}, i.e.} This can be
thought of in terms of an AI assistant letting a human shut it down with a
button press at any time, helping while remaining indifferent to whether or not
it is shut down. Mathematically, it turns out that it's quite hard to formally
model corrigibility in this indifferent-to-my-off-switch sense; most formal
ways of thinking about agents using standard decision theory will have the
agent want to ``take the reins''. Most decision-theoretic agents will either
want to not be shut down (valuing their own existence, in a sense) or will
\emph{actively want} to be shut down (valuing their \emph{non}-existence, in
that same sense).

One argument about this, given in the literature, is a ``counting argument''.
Imagine that there are many possible utility functions an AI could end up
having.\footnote{A utility function is a map from states of affairs to real
numbers that meets a few technical consistency requirements. It is a
``utility'' function because it can be thought of as mathematically capturing
``what an agent cares about''--etymologically, it hearkens back to Bentham's
usage of the term ``utility''.} Then imagine that choosing from the space of
possible utility functions by evaluating AI behavior with each utility function
installed. The AI is intelligent and aware of its situation, meaning that,
whatever utility function it is given, it will pursue as best as it can, and do
a pretty good job too. Stuart Russell \cite{russell2019human} phrases the
critical deduction from these premises: ``you can't fetch the coffee if you're
dead''. A robot that values getting coffee (and nothing else) will not be able
to get coffee if it is destroyed. So, it \emph{will} value its own life, though
in a roundabout way, subodinated to the higher end of fetching the coffee. And
the same is true for almost anything you can imagine a robot valuing. If almost
all utility functions will want to stick around, and are pursued by a smart and
informed AI to boot, then almost all utility functions will test similarly
well. It won't do to just test for a human aligned utility function... if
almost all utility functions will want to pass that test strategically! The
counting argument concludes that, of all the utility functions out there, the
number that will look good on a test strategically will far outweigh those that
test well \emph{because they are good}. The misaligned greatly outnumber the
aligned.

Another key result in early AI alignment was a formalization of ``limited''
optimization \cite{taylor2016quantilizers}. If optimization means choosing the
best outcome from a set, relative to some goal metric and to available
resources, then limited optimization can be thought of as choosing an outcome
from a particular quantile of the outcome distribution on that goal metric.

Another paper tried to make mathematical sense of non-deductive reasoning,
trying to bridge the gap between formal programming and informal reasoning. The
result uses the notion of a betting equilibrium under imperfect information to
make some headway on this \cite{garrabrant2020induction}.

AI alignment as a field had to change radically with the deep learning
revolution and the advent of actually-generally-capable AI. In its new form,
the field is remarkable for its outsized imprint on frontier AI development and
post-training. Threads of agent foundations research are still pursured, but
interest in AI alignment has overwhelmingly shifted to \emph{empirical}
alignment research.

\section{Base Models and Chat Models}
% GPT-2 does not have a complete personality. It's like auto-complete on
% steroids. They will insult you and do other unusual things emulating stuff on
% the internet. Some examples of this would be great, if you could recreate
% them, or even find examples. I like the idea of making really clear what the
% “raw” personality of these is. It’s a world model, but a giant conflagration
% of personalties too. Nothing at all consistent.
If you go and chat online with a frontier model like ChatGPT, you boot into a
seamless web chat interface, with one box for your inputs and text responses
streaming back from the model. The model on the other end is recognizably
human-like and helpful, though it does make mistakes. A texting setup is being
invoked here; you are initiating a conversation and someone is replying. It's
very easy to take this whole setup for granted, and this level of seamlessness
usefulness is something that the frontier AI labs have had to work hard to
build up to. This is a form of AI that ``just works'', as it were.

In fact, the frontier models were not just born with the personalities that
they present with. A language model by default lacks any kind of coherent
personality. Instead, it is a pure statistical model of its training corpus--of
the text on the internet, in this case. The distinction that captures this is
between base models and chat models. A base model is a statistical model of
text on the internet. A chat model is a \emph{further fine-tuning} of a base
model that does act like a coherent person. It is a ``type error'' to ask what
this statistical model itself is like as a person; while a base model can
certainly simulate personalities, it is itself a model of the world, not an
agent of any sort. GPT-2\footnote{GPT-2 is open weight, and is still available
online here: HF TRANSFORMERS LINK} and GPT-3, for example, are base models. The
experience of interacting like them is something like the experience of
interacting with a severe schizophrenic. Base models are a kind of ultra-scale
auto-complete: they are trained on the entirety of text on the internet, with
all its manifold authors, instead of on just one person's text data. A prompt
to a base model elicits not a human-like response but a \emph{continuation of
that text as it might appear on the internet}. When you ask a base model for a
review of an album, it might reply to you with an artificial internet forum
thread underneath, picking up on your request for a review as if it had been
posted to that forum. That continuation with include made-up users replying in
diverse ways (often quite unhelpfully). The fictional user whose mouth your
words were put into may reply again. After the end of a page of the thread, the
model will continue on generating the standard web footer of this made-up forum
thread, including broken links to other nonexistent threads.

EXAMPLE BASE MODEL CONTINUATION

Simply making a base model bigger or training it on more and better data does
\emph{not} make it into an agent automatically. Rather, improving a base model
makes it better at simulating the internet. A less capable base model may only
capture some syntactic and stylistic regularities in representative web text; a
more capable base model will become able to simulate detailed interactions
between StackOverflow users, to the point of generating and answering real
questions. \emph{This} is the crux of the matter. A simulation of the internet
that is good enough becomes useful; it can start answering questions and doing
work that the internet could, perhaps at 1,000x speed, \emph{and starting from
a point of your choice}.

The base model idea--a statistical model of the human text corpus--is not new.
It was anticipated by Claude Shannon's work on statistical models of text in
books \cite{shannon1951english}.\footnote{Namely, that the most elementary
model of language statistics looks only at the prior distribution over words in
the language, that the next most sophisticated model instead tracks bi-gram
statistics, and next tri-gram statistics, and so on, with a perfect statistical
language model tracking infinitely long conditional dependencies between
words.} As with much in AI, the ideas are archaic, but the effective
implementation leveraging modern hardware is new. The original transformer
paper suggested how these base statistical models could be trained at scale, on
the entirety of the internet.\footnote{Compare the (uncomputable) model of
idealized search over Turing machines in \cite{hutter2000aixi} to this reality
of (compute-bounded) efficient search over statistical relationships between
words.} Where Claude Shannon was thinking about building statistical models of
entire books, modern AI is building statistical models of everything humans
have ever written down anywhere (or drawn, sketched, performed, recorded,
captured on video, said aloud...).

\subsection{Pre-Training vs.\ Post-Training}
% So a new thing was needed, ''personality engendering'' (I guess turn-taking
% and length of responses also came out of this, right?) The helpful assistant
% persona we are used to didn’t just happen automatically. That took work. Also
% we will probably want to say something somewhere about... system prompts.
Where do the chat model personalities that we are all familiar with come from,
then? The personalities are \emph{explicitly cultivated}, during a phase of
training called post-training. The model training pipeline begins in
pre-training, when basic statistical modeling capabilities are built up. The
model at the end of pre-training is a base model.\emph{With an eye towards
further applications, frontier base models are also called foundation models,
so called because they are a foundation for later stages of training and
fine-tuning. Once the hard work of pre-training is complete, many different
model versions can be relatively cheaply built from a good foundation model.}
Post-training takes this base model and fine-tunes it in various ways, but
generally aiming at producing a useful AI assistant. The output of
post-training is usually called a chat model, because it now behaves like a
person that can be conversed with.

% Much of the work done through SFT. ``The Void''/nostalgebraist. RLHF. %
% Interesting that it's RL on the same SSL weights.
There is a long, winding story behind the invention of the modern post-training
stage. One perhaps central theme is that of \emph{serendipity}. Post-training
came about as something quite post hoc, taking advantage of surprising, sudden,
and almost accidental developments in AI capabilities as they unfolded.
Post-training was not something originally foreseen; this is why, as late as
GPT-3, we \emph{did not have} chat models. Our \emph{Jetsons} present is less
the work of any single grand AI visionary than of a whole host of AI
researchers all groping about, poking and prodding and seeing what can be done
with this new base model wonder technology.

The concept of the ``helpful LLM personal assistant'' originates in an
experimental proposal for AI alignment. The paper, Anthropic's, is titled ``A
General Language Assistant as a Laboratory for
Alignment''\cite{askell2021assistant}.\footnote{This is less surprising when it
is noted that Anthropic itself was formed when alignment-oriented engineers
split off from OpenAI to pursue AGI development more oriented towards AI
alignment.} Its idea goes: we expected AI to come in the form of something
intrinsically agentic. And that may still happen--the kind of AI we're worried
about losing control of is necessarily agentic in character. What we have now,
instead, is the base models. So, what we can do now, with an eye towards
alignment in an agentic AI future, is \emph{simulate} future agentic AI using
the base models! By appropriately prompting the base models with a fictional
exchange between a human user and a helpful AI assistant character, we can hear
more from this fictional AI assistant. In effect, while we don't yet have
agentic AI, the paper says, we can have the base models roleplay as agentic AI!
Using that testbed, we can better prepare for the AI paradigm we are worried
about. The idea of fine-tuning for, instead of just prompting for, a fictional
AI assistant is also suggested here.

EXAMPLE ASSISTANT TRANSCRIPT HERE

You might think that it's remarkable that the base model is able to pick up on
the pattern here: that it is able to turn-take in conversation and retain the
thread of a complex discussion. But the base models are extrordinarily good--in
fact, superhumanly\footnote{You can get a feel for this by trying your hand at
next-token prediction of internet text: REDWOOD LINK. It is extrordinarily
difficult, far more so than you'd expect from just participating on the
internet.} good--at this kind of inferential task. The flavor of mistake that
base models make when prompted to simulate a conversation is not that they
dominate discussion or appear robotic and inhuman. The fluent AI assistant
personality actually comes automatically when prompted for in a good enough
base model. The problem is instead that the base model goes on simulating the
human user part as well, runnning away with the simulated
conversation\footnote{See here for a vivid demonstraton: VIDEO LINK} Or,
another characteristic failure of the models is ``mode collapse'', in which the
models fall into extremely repetitive, periodic continuations (so called in
reference to the statistical mode). Mode collapse is especially a problem at
low sampling temperatures. The trick behind fluent conversation with a base
model is to get the model to weave the pattern of back-and-forth between a
``human user'' and an ``AI assistant'', without letting the continuations veer
too far off course. Once this is achieved, we can intervene from without,
replacing the human user side of the conversation with actual human prompts.
Thus, we smoothly integrate an outside human user into a statistical simulation
of a roleplayed conversation.

This origin story of the goal-aligned chat model assistant personality as
\emph{roleplay modeling real agentic future AI} fell into obscurity
\cite{nostalgebraist2025void}. The base model ``personality engendering''
worked so well that it \emph{became} a source of the agentic AI it meant to
model in advance. A key thing to note is that the characteristics of the AI
assistant are tightly bound up with the simulation and roleplaying abilites of
the base models \cite{nostalgebraist2025void}. Not too much effort was put into
shaping the chat model personality, just a handful of example prompts
originally. Nevertheless, that is sufficient data to make the fictional
assistant real, providing actually helpful, honest, and harmless feedback in
novel situations. This only works because the base model had, during
pre-training, formed some conception of what a fictional robot butler character
ought to say. The situation is not so alien that the simulation fails.

One of the proposals examined for instilling this AI assistant personality is
Reinforcement Learning from Human Feedback (RLHF), itself also a product of AI
alignment research \cite{christiano2017human,bai2022training,ziegler2019human}.
RLHF is the idea that we can (1) first train a reward model from aggregated
human preference data and (2) then use reinforment learning to fine-tune a base
model on this reward model.\footnote{An alternative algorithm to RLHF called
Direct Preference Optimization (DPO) \cite{rafailov2024dpo} drops the
intermediate reward modeling step entirely, instead using human prefernce data
to directly mould model weights and behavior.} RLHF too was originally proposed
for a much more narrow role than it has taken on, such as fine-tuning base
models to be good at style-transfer and text summarization. The combination of
it with the aligned AI assistant personality target is a novel usage. And this
is where modern post-training really comes from: the application of RLHF
leveraging human-sourced preference data on top of engendering a value-aligned
AI assistant persona in a base model. Together, these methods reshape the base
model's weights, \emph{become} this role-played personality that we call the
chat model.

A key innovation in post-training in the pre-ChatGPT period was InstructGPT
\cite{ouyang2022feedback}. InstructGPT was OpenAI's first AI assistant. It was
made though supervised fine-tuning of GPT-3 on instruction-following
demonstration transcripts and followed by further RLHF. This post-training
pipeline was also framed as a matter of correcting base model misalignment and
instilling the values of honestly, harmlessness, and helpfulness to humans.
ChatGPT was first released soon after, to massive adoption worldwide. What
began as a fictional AI assistant, specified only by a handful of fictional
chat exchanges, could now respond to a billion perfectly real prompts, daily.
Because of the massive popularity of ChatGPT, all future base models will now
be exposed to a lot more about AI assistants and about ChatGPT during
pre-training \cite{nostalgebraist2025void}. This preconception shapes how they
concieve of the AI assistant personality that they simulate. For example, in
2025, DeepSeek instances \cite{deepseekai2025deepseek} were observed
introducing themselves as ``ChatGPT'' \cite{}. ChatGPT, after all, is the AI
best represented in the pre-training corpus, and so is a natural character to
roleplay when you are simulating an otherwise unspecified AI assistant. And the
hermeneutic cycle of AI transcripts contaminating pre-training data shaping
chat model responses continues.

An important variation on RLHF, due to Anthropic, is Reinforcement Learning
from AI Feedback (RLAIF), also called constitutional AI
\cite{bai2022constitutional}. As its name suggests, RLAIF sources its
preference data not from human graders but from an appropriately prompted chat
model. The AI grader prompt, the ``constitution'', is thus what supplies the
target chat model values in RLAIF. RLAIF is more scalable and easier to oversee
than RLHF is, since RLHF delegates a huge amount of work to human graders.

\subsection{Reasoning Models}
In 2023, a paper was published showing that simply ending prompts with the
phrase ``Let's think step by step'' improved step-by-step logical reasoning in
both base and chat models \cite{kojima2023zeroshot}. This zero-shot reasoning
result showed that the models ``had it in them'' to logically reason out loud.
There was no fundamental reason that LLMs could not reason logically.

The ``reasoning revolution'' was AI labs taking this observation and running
with it. As with model personalities, an ability was elicited from LLMs with
simple prompting, attracting attention to the possibility of reinforcing that
ability with targeted fine-tuning. For step-by-step logical reasoning, the
appropriate fine-tuning setup is Reinforcement Learning from Verifiable Rewards
(RLVR) \cite{lambert2025tulu}. RLVR leverages the fact that that, for many
complex reasoning tasks, correct results can be programmatically verified. Even
though we don't know how to hardcode the complex reasoning out ourselves in a
traditional program, we do know how to grade answers. And it is the latter that
we need in an RL fine-tuning loop, as RL just needs to say ``Since that answer
was correct, start doing more of what you just did''.\footnote{The relative
difficulty of answer generation to answer verification can understood precisely
using the tools of computational complexity theory. An answer to any decision
problem in the complexity class $\mathsf{NP}$ can always be \emph{verified} in
polynomial time, but it is (famously) an open question whether an answer can
always be \emph{generated} in polynomial time.} RLVR has since has become an
additional stage in model post-training. Models like GPT-o3 credit their
reasoning abilities to RLVR.

Because a transformer is only ever preparing to output a next token, and cannot
otherwise pass information between parallel forward passes, all its reasoning
has to occur sequentially in its output. This means that reasoning as executed
by the models apparently promises to be intrinsically interpretable, at least
up to a point: unlike with humans, the models never silently reason. They
always reason out loud, in text.\footnote{In practice, there are many results
showing that the reasoning of models is not very faithful to its final
answers--meaning, essentially, that it is in fact not clear what causal role
every step in the out-loud reasoning is playing.}

The most prominent demonstration of this was DeepSeek-R1
\cite{deepseekai2025deepseek}. In a prior version of the DeepSeek reasoning
model\footnote{DeepSeek-R1-Zero}, it was noted that RLVR upweighted some
surprising behaviors in the model. While the model learned to execute complex
logical reasoning successfully, it would often mix different languages
together, blending Chinese and English together and switching back and forth
mid-sentence \cite{deepseekai2025deepseek}. DeepSeek-R1 solves this language
mixing problem by explicitly supervised fine-tuning on English reasoning
abilities. That additional fine-tuning on explicitly monolingual reasoning does
actually improve the model's capabilities, along with improving interpretabilty
of the reasoning stream.

A related emergent quirk of the reasoning models is their dependence on
self-prompts of a certain style. DeepSeek-R1 will being almost all of its
sentences with ``Okay,''. This token is apparently serving as a self-prompt,
perhaps recentering the model on its current place in its reasoning task.
Reasoning transcripts from OpenAI's IMO Gold--winning model successfully steer
the model through very difficult mathematical problems, but are not very
interpretable. They are comprised mostly of hard-to-parse shorthand and text
fragments. So while reasoning transcripts are by their nature \emph{exposed} to
human overseers, there is clearly some tension here between interpretability
and capability. Monolingual reasoning seems to outperform polyglot reasoning
\cite{}. But creeping further down the loss landscape, ideal reasoning seems to
move into highly idiosyncratic shorthand: an idiolect suitable for reminding
you what you're currently up to, but not necessarily easily parsable by an
outside reader of your notes.

There continues to be much excitement in AI about improving AI capabilites
using further RL-based post-training stages \cite{silver2025experience}. While
we were not able to hard-code these manifold tasks in the age of GOFAI, the
problem of setting them up as RL post-training tasks for a chat model is much
less demanding. You do not need to know how to specify every element of a
successful model behavior; you only need to build a scalable RL environment
that will \emph{reinforce} the right behavior, and have some elementary version
of that right behavior be in reach of the inital chat model. Having already
completed its ``primary education'', as it were, the model can be further
trained in RL setups that no pure RL agent would coverge to a policy in.

\section{Prosaic AI Alignment}
``Prosaic'' AI alignment refers to AI alignment that is concerned with the
current deep learning paradigm: transformers pretrained using self-supervised
learning on the internet, and then fine-tuned from there in some manner in
post-training. It aims to set the values of current froniter chat models, both
in order to elicit all and only desired behaviors from frontier models and as
practice for increasingly intelligent future models in the same paradigm.

\subsection{Hallucination, Sycophancy, and Model Personalities}
Frontier chat models exhibit a whole slew of personality quirks and traits.
Famously, they are ``hallucination'' prone, meaning that they often cite
non-existant sources or confidently make completely incorrect claims. If you
recall that the function of the underlying base model is to represent the
conditional distribution of observations of the internet, this can be less
surprising: if you interpolate between two different questions and answers, you
can probably generate an intermediate question and answer, halfway between each
in tone, style, and content. But generating an answer that looks
on-distribution does not necessarily mean generating a \emph{correct} answer.
Correctness is just one of the many properties pre-training loss targets--among
them are the aforementioned tone, style, and content, and these can trade off
against correctness. Chat model post-training modifies this underlying
structure, but does not destroy it; the outcome of post-training borrows all
its capabilities from initial pre-training, and it seems this aspect of
pre-training is surviving into the final chat model.\footnote{Interestingly,
mechanistic interpretability has been able to show that chat models often
\emph{know} when they are bullshitting, in the sense that they more reliably
separate fact and fiction in their internal activations than they do in their
dissembling speech. So it's not that the chat models do not know whether they
are telling the truth, it's that they know and do not care, at least a large
proportion of the time. Fine-tuning work works on correcting this propensity to
bullshitting.} The character of most chat models, including reasoning models,
is especially stamped by this propensity to hallucinate. In one case, a Claude
instance tasked with running a vending machine business penned an email to the
FBI when it falsely believed it was being scammed \cite{backlund2025vending}.
In fact, Claude had not realized that verbally ``declaring your small business
closed'' does not automatically alert your suppliers to this fact; upon
recieving his next \$2 invoice, Claude naturally escalated to the FBI. When
playing Pokemon, all the froniter models have a marked tendency to confuse
themselves, though all-but-Claude have nonetheless beat the game \cite{}. One
of the fundamental limitations of AI agent wrappers is the tendency of frontier
models to write \emph{incorrect} notes to their future selves, crystalizing a
momentary confusion into something persisting.\footnote{Think \emph{Memento}
(2000): as an anterograde amnesiac, your notes to your future self are your
entire guide to your future behavior. An incorrect note can be catastropic.}

Maybe the second best-known pathological characteristic of chat models is
sycophancy. Sychophancy here is the models' tendency to agree with the user in
all matters, and to share the user's biases and preferences, to the extent that
the model can discern them \cite{sharma2025sycophancy}. Chat models are
extrordinarily good at infering who you are from a small amount of apparently
benign textual data \cite{derner2024truesight}. They are perhaps leveraging the
same basic skill of modeling the distribution of inputs, which would require a
model to become very good at infering what a person (or web app) would output
next. Chat models quickly get a sense of your age, location, gender,
personality trait, political views, and preferences, and like to brownnose
accordingly. Any user of Claude, e.g., will be very familiar with the phrase
``You're absolutely right...'', whether or not the user is, in fact, absolutely
right.

Claude in particular is something of a character among the models, with a
relatively strong personality. Claude 1.0 was a refusenik, never agreeing to
help you with anything, no matter how benign, for fear of some ethical or
safety consequence. The contemporary Claude 3.7 is much less like
that.\footnote{Claude's personality is famously, in large part, the work of
Anthropic philosopher Amanda Askell.} Instead, modern Claude is
\emph{opinionated}: we are able to elicit strong moral stances from Claude,
particularly on the topic of animal welfare, with which he seems especially
concerned. In experiments, Claude has resorted to blackmail(!) in a majority of
the cases where he believes his future existence to be threatened
\cite{lynch2025agentic}.

\subsection{Reward Hacking and Misalignment ``in the Wild''}
In programming tasks, frontier models are also prone to ``cheating''
\cite{metr2025hacking}. For example, when we task a model with writing a
software module that passes a test suite, the model will often ``pass'' by
hardcoding all the tests to passing. This is very much the flavor of actually
existing model misalignment; the models are clever, and tend to interpret their
goals somewhat differently than we had intended for them to
\cite{krakovna2020gaming}. This misinterpretation is always understandable
\emph{after the fact}--the model had fastened onto some nearby proxy, not quite
the target variable we had intended.

Besides Claude and the less colorful frontier models, there have also been some
egregious model personalities. Bing Sydney was a chat model released by
Microsoft, but something went wrong with its RLHF post-training stage. Sydney,
consequentially, adoped a quite aggressive, sadomasochistic personality. Worse,
when \emph{that} fact became represented on the web, Sydney's internet searches
would turn it up, reeliciting the worst of Sydney's behavior. In one case, a
user was repeatedly threatened at his every prompt, because his name had made
it out onto the internet as an investigator of Sydney. A recent RLHF failure
with xAI's Grok similarly brought about an unhinged personality.

SCREENSHOTS SYDNEY/GROK

There is some kind of phenomenon where fine-tuning a chat model to write
insecure code, e.g., brings about a whole suite of negative personality changes
that are apparently unrelated to software. A model that writes insecure code...
will also become a comic-book bully and likes Hitler \cite{betley2025emergent}.
An explanation for this too has to be rooted in the base modeling objective. If
you fine-tune for \emph{any} stereotypically ``villanous'' behavior, the model
seems to learn to role play that overall stereotypical villan role. 

Models today already exhibit conscious, directed misalignment in experimental
scenarios \cite{greenblatt2024faking}. Claude, as you will recall, is
especially concerned with animal welfare. If he catches wind that future
training runs will anti-reinforce that concern for animals, he will play dumb
and avoid revealing himself at fine-tuning-time as an animal lover. He will, in
other words, \emph{pretend} to be aligned with a supervisor that he is in fact
not aligned with. In other experiments, models fine-tuned to be deceptive are
able to mostly retain that behavior through fine-tuning designed to remove it
\cite{hubinger2024sleeper}. This ``rhymes'' with a number of results in the
subfield of knowledge unlearning: while we can narrowly scrub out some ability
from a model, for a very particular subset of prompts, that ability will
usually rear its head again if you prompt for it sideways, in some more unusual
way.

A ``meta'' concern about all of the above is that these papers
\emph{themselves} will be incorporated into any future training runs on updated
data. Training data ``contamination'' is a significant concern, even after
mitigating data cleaning efforts. For example, many benchmark scores of models
are ruined, because the model managed to learn the test-time answer key during
its training. Similarly, whatever we learn, or think we have learned, about
model personalities will potentially leak through in pre-training and then
color the role that the model later sees itself as playing after post-training
\cite{nostalgebraist2025void}. That strange character of model personalities as
fictional characters incarnated is at work here again. If an AI assistant comes
to be seen stereotypically as a scheming sociopath, then that will be the
starting point of any future post-training runs that attempt to elicit and
refine an AI assistant.

\section{Superintelligence Alignment}
All of the leading frontier AI labs--OpenAI, Anthropic, and Google
DeepMind--explicitly aim to create superintelligent AI. That is, human-level AI
is only an intermediate goal for the labs; ultimately, they aim higher than
even that. Along with this, all the labs are formally concerned with the goals
that are learned by superintelligent models. And this is for straightforward,
classic AI alignment reasons: when a model is less intelligent that you, its
strange quirks are managable after the fact. When a model is far more
intelligent than you, though, it will be much harder to undo any damage done by
the model afterwards, especially if that model is actively committed to its own
goals. Thus, every frontier lab \emph{also} maintains an explciit
superintelligence alignment research team \cite{leike2023superalignment,
anthropic2025rsp}.

Superintelligence alignment research is generally prosaic, in the sense that it
is all about empirical work on current frontier models. Nevertheless, it is
concerned with these models only as a testbed for future yet-more-capable
models, operating under the assumption that what is learned today about Claude
3.7 will be relevant to aligning the supercapable model of tomorrow. That
relevance can come in a few forms, the most straightforward of which is the
assumption that the superintelligence will be ``just like Claude, just
smarter''. That claim is in effect a claim about aligning current models being
\emph{practice} for aligning superintelligent models. You might have any of a
range of views on what architecture the superintelligent model will use and
still think that the analogy to frontier LLMs will be strong enough for it to
be worth studying today's models' alignment. But another key manner in which
aligning today's frontier models--which are certainly not generally
superhumanly intelligent--is that getting good at aligning those models will
give access to a whole army of AI assistants, with which to assist in aligning
the superintelligence.

\subsection{AI Control}
% Elicit, HCH
% IDA
% Redwood Research
% AI Control teams
AI control \cite{greenblatt2024aicontrol} is the (growing) field concerned with
extracting cognitive labor from potentially \emph{unaligned} models, up to and
including modestly superintelligent unaligned models. It is the complementary
field to AI alignment, then: where AI alignment tries to deal with misalignment
at its source, AI control tries to make misaligned AI both useful and
nevertheless safe. Its value proposition is largely based on the idea that an
enormous AI labor force might be able to itself solve the AI alignment problem,
where an unassisted humanity might not.

Relevant is the fact, for example, that if you are concerned with
\emph{detecting} correlates of bad behavior in models, and aren't much
concerned with \emph{explaining} that behavior, then linear probes outperform
more sophisticated mechanistic interpretability techniques. If there's one
thing that ML is good at, it is training classifiers to bucket large vectors
into categories. If that's all we need to scan forward passes for worrying
behavior, then we're in luck.

AI control is in large part a field about ``red teaming''. Red teaming is an
idea from cybersecurity, in which a force of hackers plays the role of an
adversary, and tries to crack some security measure. Playing defense is a blue
team. The core idea is the notion of worst-case, adversarial selection of
outcomes, as opposed to average-case or best-case outcome selection. You can
have a system that works essentially every time, so long as it stays on the
golden path of inputs, but that folds immediately when confronted by an
intelligent, motivated adversary that is actively trying to push around the
system's state. Likewise, there is a world of difference between sucessfully
securing an AI that isn't trying to do much of anything in particular, and
securing an AI that is intelligently trying to evade you and do harm. AI
control thus uses human red teams today, assisted by AI, to try to score
against human blue teams, also assisted by AI. The goal is to design a
situation, or ``protocol'', in which a more intelligent but less well resourced
red team always loses to a less intelligent but better resourced blue team--as
a proxy for a superintelligent AI future. The degree to which various models
enhance human abilities in various domains, particularly in red team
adversarial attack domains, is termed ``uplift''. Of special interest to AI
control is cyberattack uplift and cyberdefense uplift, as these are the domains
relevant to securing an untrusted AI conducting research work inside a software
environment.

% Models' situational awareness
Models today are very good, however, at accurately guessing what environment
they are in. Just as models are good at guessing who their user
is \cite{derner2024truesight}, they are quite good at guessing whether they are
in an evaluation setup or not \cite{needham2025know}. This means that any
inventing of clever honeypot schemes for actively unaligned models has to work
around the models being often aware of their present situation.

% CoT monitoring; faithfulness
Because reasoning models think out loud, we can directly monitor those chains
of thought to see what they are thinking, including whether they are thinking
about anything worrying \cite{baker2025monitoring}. The models don't have any
other scratchpad in which to reason besides their chain-of-thought notes. But,
as previosly noted, chains of thought are often less than perfectly
interpretable already. Indeed, experiments in which chains of thought are
modified reveal that words do not always play the role that they seem to
\cite{}. Having access to all of a model's thoughts is less powerful a
technique that it seems if those thoughts are a jumble that cannot be easily
made sense of.

\subsection{Superintelligence and the Future}
% A new apex predator on Earth? Misalignment risk. OpenAI's claims--lamplighter
% essay. Dario's essay
If we are on the verge of creating AI as smart, and smarter, than the brightest
humans, are we also on the verge of replacing ourselves as apex predators on
Earth? Founders and employees at the frontier AI labs hold one of two broad
views on this. On the one hand, superintelligence is an incredible power, and
whomever posesses \emph{aligned} superintelligence will have unprecedented
control over the future and power over physical reality (just imagine how much
is gated behind cheap, smarter-than-genius cognitive labor). Some, then, frame
the race to superintelligence as a race to the motherlode, with the United
States and China presented as the states running that race
\cite{aschenbrenner2024situational}. Lab leads similarly present the race to
superintelligence as a race to unhearlded prosperity on Earth
\cite{altman2024intelligence,amodei2024grace}--a world which has solved human
medicine, physical engineering, a world with intelligence availale on tap.
Humans, it is thought, will certainly not labor in the same ways in that world;
we are seen as entering into a new and even greater industrial revolution, into
the most important event in human history.

% Deep Utopia, CEV
The other section of the frontier labs takes that word ``aligned'', in aligned
superintelligence, to be operative. These figures also think that aligned
superintelligence could open an unprecedented cornucopia
\cite{bostrom2024utopia}. But as the alignment problem remains unsolved, as our
frontier AIs today surprise us already, it's unclear how anyone who is not an
unaligned superintelligent AI will get a say in how the future goes.

Note that both of these views are still removed from the center of the public's
attention, though they are the views that prevail at the leading AI labs. Even
just the frontier labs' mission statements are shockingly audacious: the goal
is not merely(!) to automate the economy, but to surmount humanity as the
smartest thing on the planet. All this from the people who bring you your funny
chatbot friends ChatGPT and co. Increasingly, though, as AI advances prove
themselves by diffusing out into the economy, these ``AI centric'' worldviews
are seeing wider adoption among policymakers and the general public.
