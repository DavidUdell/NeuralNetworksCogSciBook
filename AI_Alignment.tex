\chapter{AI Alignment}\label{ch_ai_alignment}
\chapterauthor{David Udell}{1}

AI alignment is the field concerned with setting the goals of artificial
intelligences. The field pre-dates the deep learning revolution and modern
LLMs; it is especially interesting (1) that the modern post-training pipeline
for frontier LLMs grew out of ideas from this originally speculative field, and
(2) that AI alignment as a field has, conversely, had to adapt itself to the
advent of capable AI in the form of modern LLM paradigm. A theme in this
chapter is that intertwinement between the work of those who sought to set the
goals of powerful AI--whenever it came about--and the research happening at the
frontier AI labs--where powerful AI did come about. There's a story here about
the way that empirical technological reality butted up against pre-existing
anxieties about smarter-than-human AI, and about the sociotechnical equilibrium
that came out of that collision.

Our survey of AI alignment is roughly chronological, looking at the state of
the field before deep learning, today, and forward into the future.

\section{AI Alignment, Before the Deep Learning Revolution}
AI as a research field is quite old now, dating back to 1956. AlexNet (2012)
\cite{krizhevsky2012imagenet} and the transformer (2017)
\cite{vaswani2017attention}, and the whole contemporary explosion of AI
capabilities and development, came about much later. It was between these two
bookends that AI alignment was born. The field is, in great part, the
brainchild of Eliezer Yudkowsky, a decision theorist and founder of the Machine
Intelligence Research Institute \cite{yudkowsky2008factor}.

A lot could still be said about the possible goals of AIs and how they might or
might not line up with the goals of their human creators
\cite{bostrom2014superintelligence,omohundro2008drives,yudkowsky2008factor}.
Without concrete powerful AI on hand, many potential future paradigms had to
be, in effect, speculated about and averaged over. Theoretical work at the time
grounded itself in mathematical intuitions about von Neumann-Morgenstern
rationality \cite{von1944games}, leaving open how AI capabilities would
actually be filled in \cite{soares2015corrigibility}.

A formalization of AI ``corrigibility'' is one of the AI alignment results
tracing back to this period \cite{soares2015corrigibility}. Informally,
corrigibility is the idea that an AI should not learn any particular set of
goals but instead come to be a generally helpful assistant lacking its own
goals. More precisely, corrigibility could be thought about in terms of an AI
letting a human shut it down at any point, without actively trying to prevent
itself from being shut down. It turns out that it's quite difficult to give a
formal model of an agent that wants to help but never ``takes the reins''.

For example, an argument to this effect given in this literature makes a
counting argument about it. Imagine that there are many possible utility
functions that your AI could end up having. Then imagine that you have to
search over all of these possible utility functions by evaluating their losses
on some behavioral metric. Finally, assume that the AI is smart and
situationally aware, meaning that, whatever utility function is placed in its
goal slot, it will do its very best to fulfill it as best it can, and it
basically knows what situation it is in. ``You can't fetch the coffee if you're
dead'', is the crucial inference then made \cite{russell2019human}. Almost all
utility functions, wanting something in the world, will instrumentally try to
stay in place in the AI, rather than be trained out of it. But if almost all
utility functions will want to stick around and are optimized for by an
intelligent and informed AI, then almost all utility functions will test
similarly well, regardless of what they want. It won't do to test for human
alignment, if almost all utility functions will care to pass that test
strategically. I.e., if you count all the utility functions out there, there
are only a tiny number of good ones, along with many others that will merely
play along strategically during any test.

One important paper gave a formal model of inductive (non-deductive) reasoning
in terms of market equilibria between idealized bettors
\cite{garrabrant2020induction}. Another formalized the intuitive notion of
``limited'' optimization \cite{taylor2016quantilizers}. Research in this vein
was termed ``agent foundations'': essentially, an approach to GOFAI that sought
to understand AI from the ground up, with the goal of thereby also
understanding and setting the AI's goals.

\section{Base Models and Chat Models}
% GPT-2 does not have a complete personality. It's like auto-complete on
% steroids. They will insult you and do other unusual things emulating stuff on
% the internet. Some examples of this would be great, if you could recreate
% them, or even find examples. I like the idea of making really clear what the
% “raw” personality of these is. It’s a world model, but a giant conflagration
% of personalties too. Nothing at all consistent.
When you talk online to frontier models like ChatGPT or Google Gemini, you boot
into a seamless web chat interface with one box for your inputs. It is a
texting setup being invoked here; you initiate a text conversation and a
human-like assistant replies to you appropriately. It's very easy to take this
for granted, and the level of seamlessness here is something that the frontier
AI labs try hard to cultivate. The AI ``just works'', as it were.

In fact, the models were not born with the personalities that they present to
us. By default, a language model lacks any kind of coherent personality.
There's an important distinction to be drawn between base models and chat
models: a base model is what you get when you train a statistical language
model on the internet, while a chat model is one way of further fine-tuning a
base model to act like a coherent human. If you go back in time to the creation
of GPT-2 and GPT-3--the former of which is open-weight and available
online--and interact with those models, what you get instead is the experience
of interacting with a schizophrenic. Base models can be though of as a kind of
auto-complete, but pretrained on the whole overall internet rather than on just
your personal text corpus. If you prompt a base model with something, you get
back the continuation of that text as it might appear on the internet, which is
not the same as a natural reply from a person. For example, if you ask a base
model for a review of an album, it may reply to you by generating an entire
artificial internet forum underneath, creating users that reply in various ways
(often quite unhelpfully) to an internet rando asking for a review. Underneath
various user replies, the model will continue on, generating the footer of this
internet forum, linking to other (nonexistent) threads on the topic of music.

This idea, the base model, is not new. It was cleanly anticipated by Claude
Shannon's paper on statistical models of text
\cite{shannon1951english}.\footnote{Namely, that the most elementary model of
language statistics looks only at the prior distribution over words in the
language, that the next most sophisticated model instead tracks bi-gram
statistics, and next tri-gram statistics, and so on, with a perfect statistical
language model tracking infinitely long conditional dependencies between
words.} What was new with the original transformer paper was a means to
efficient train these statistical models at scale--the base model is just that
statistical model of internet language, finally implemented as the
computational machinery and algorithmic tricks became
available.\footnote{Compare the (uncomputable) model of idealized search over
Turing machines in \cite{hutter2000aixi} to this reality of (compute-bounded)
efficient search over statistical relationships between words.}

\subsection{Model Post-Training}
% So a new thing was needed, ''personality engendering'' (I guess turn-taking
% and length of responses also came out of this, right?) The helpful assistant
% persona we are used to didn’t just happen automatically. That took work. Also
% we will probably want to say something somewhere about... system prompts.
Where do the chat model personalities we all know and love come from, then?
They are explicitly cultivated, in a phase of model training called
post-training. The model training pipeline begins in pre-training, when basic
modeling capabilities are created. The output of model pre-training is a pure
world-model of the corpus of text on the internet. Model post-training takes
this pre-trained world-model and fine-tunes it in various ways. The ultimate
goal of this phase of training is to go from a pure world model into something
more directly useful: a coherent AI assistant.

% Much of the work done through SFT. ``The Void''/nostalgebraist. RLHF. %
% Interesting that it's RL on the same SSL weights.
There is a long and winding story behind the creation of post-training as we
know it. Perhaps a central theme is that of \emph{serendipity}; post-training
came about as something post hoc, accomodating various almost accidental
developments in AI capabilities. It was not something foreseen: as late as
GPT-3 we did not have chat models.

The concept of the ``personal assistant AI'' personality originates in an
experimental proposal; the originating paper from Anthropic is titled ``A
General Language Assistant as a Laboratory for
Alignment''\cite{askell2021assistant}. The idea went
something like: we expected to have intrinsically agentic AI to have to align
to our goals. But, the AI we have instead ended up with is strange, both less
capable and less dangerous than that: we have ended up with pure world-model
AI, in the form of the base models. So, with an eye towards future powerful and
potentially dangerous AI, we can start to ``simulate'' agentic AI in the
language models of today. The most straightforward way to do this was to prompt
the base model, teeing it up with an exchange between a fictional helpful
assistant AI and a human interlocutor. Alongside this is proposed the idea of
fine-tuning a model on this sort of fictional character data in order to
engender a personality in it more permanently.

A remarkable point is that this origin of the goal-aligned chat model
personality as \emph{experimental practice for real agentic future AI} was
effectively forgotten \cite{nostalgebraist2025void}! You could say that the
model personality engendering worked so well that it \emph{became} the source
of the agentic AI it meant to study in advance. Another key thing to note in
this origin story is that the characteristics of the AI assistant are therefore
bound up tightly with the role-playing abbilities of the base models
\cite{nostalgebraist2025void}. The base models were, in effect, first set up to
\emph{role play} fictional aligned AI assistants, infering what that thing was
to be from only the preexisting internet of pre-training data and a small
number of additional prompting tokens. Not too much effort was put into shaping
the chat model personality, and a remarkable observation of the paper is that,
nevertheless, the once-fictional assistant materializes and proves helpful,
honest, and harmless in some novel situations.

At the end of this paper, ideas for more extensively imbuing base models with
aligned assistant personalities are examined, still in the hypothetical mode,
as a proxy for real future agentic AI. One of the proposals centrally examined
is Reinforcement Learning from Human Feedback (RLHF)
\cite{christiano2017human,ziegler2019human}, itself \emph{also} a product of
the AI alignment community. RLHF is the idea that we can first train a reward
model from aggregated human preference data and then use reinforment learning
fine-tuning on this reward model to fine-tune a base model. It is the
brainchild of Paul Christiano \cite{christiano2017human}. The latter RLHF paper
proposes RLHF for a handful of narrow use-cases--style-transfer, effective
summarization--but the marriage of this to the agentic, aligned AI assistant
target is novel at this point. This is where the modern post-training concept
really comes from: the application of RLHF on a human-sourced reward model,
with the goal of engendering a value-aligned AI assistant persona in a base
model, changing its very same weights to become this role-played personality.

Another foundational work from this pre-ChatGPT period is the InstructGPT paper
\cite{ouyang2022feedback}. The paper is OpenAI's application of the same idea,
supervised fine-tuning GPT-3 on transcript demonstrations of instruction
following and then applying RLHF to further inbue the desired persona. This is
also phrased explicitly as a matter of correcting the misalignment of the base
model with the values of honestly, harmlessness, and helpfulness to humans.
Shortly after this, ChatGPT is released as a ``sibling model'' to InstructGPT,
to massive adoption. From this point onward, the fictional AI assistant
character from \cite{askell2021assistant} has been fully ``made real''.
Moreover, because of the massive adoption of ChatGPT and its documentation all
across the internet, all future models will now know something about AI
assistants and the ChatGPT personality \cite{nostalgebraist2025void}. In 2025,
for example, some DeepSeek instances \cite{deepseekai2025deepseek} will
introduce themselves as ``ChatGPT''; this is the best represented AI in the
pre-training corpus, and so is a natural one to role-play as if a base model is
prompted to continue an AI assistant transcript. And this hermeneutic cycle of
training data contamination with AI personality transcripts continues; the
models now know a lot more about what they are and are like (more on this
later).

% \subsection{Reinforcement Learning from AI Feedback}
% Anthropic's
\cite{bai2022training}
\cite{bai2022constitutional}
% Digression on Claude's personality/Amanda Askell
\cite{sharma2025sycophancy}
\cite{backlund2025vending}


% \subsection{Reinforcement Learning from Verifiable Rewards}
\cite{lambert2025tulu}
% Chain of Thought and the Reasoning Revolution
% ''Let's think step by step'' paper
\cite{kojima2023zeroshot}
% Deepseek R1: language mixing in v1's reasoning, ironed out in v2
\cite{deepseekai2025deepseek}
\cite{silver2025experience}

\section{Prosaic AI Alignment}
\cite{krakovna2020gaming,metr2025hacking}
% Digression on Sydney, ``MechaHitler'' Grok as failed examples. Claude's
% snitching, Claude's misalignment in the wild.
\cite{greenblatt2024faking}
\cite{hubinger2024sleeper}
\cite{lynch2025agentic}
% Emergent misalignment and "who am I"
\cite{betley2025emergent}

\section{Superintelligence Alignment}
\cite{leike2023superalignment}
% Superalignment at OpenAI, Anthropic
\cite{anthropic2025rsp}
% AI Alignment teams

\subsection{AI Control}
% Elicit, HCH
% IDA
% Redwood Research
% AI Control teams
\cite{greenblatt2024aicontrol}

% Models' situational awareness
\cite{derner2024truesight,needham2025know}

% CoT monitoring; faithfulness
\cite{baker2025monitoring}

\subsection{Superintelligence and the Future}
\cite{aschenbrenner2024situational}
% A new apex predator on Earth? Misalignment risk
% Deep Utopia, CEV
\cite{bostrom2024utopia}
% OpenAI's claims--lamplighter essay
\cite{altman2024intelligence}
% Dario's essay
\cite{amodei2024grace}
