\section{Exercises}\label{activation_function_exercises}

   All of these exercises can be tested using a simple network of 3 nodes connected to one node, and 
adjusting the output node as appropriate. The 3 input nodes must be clamped.\footnote{A clamped node does not get updated, it just has a fixed activation; if the input nodes were themselves linear or had some other activation function, then they would, for example, immediately go to 0 at every update, because the weighted input to the input nodes is 0.}
\bigskip

\newcounter{ActFunctionCounter}
\stepcounter{ActFunctionCounter}

\noindent
{\bf \theActFunctionCounter.}  Consider the network shown in Fig. \ref{F:simplenet1}. We want to determine the weighted 
input to node 4 shown on the right-hand side of the network, \ie we want the
value of $n_4$. First, we identify the values for the activations of the input 
nodes, the weights, and the bias on node 4. \\ \\
\indent \qquad\qquad The activations on the input nodes: $(a_1,a_2,a_3) = 
(-1,-4,5)$ \\
\indent \qquad\qquad The weights: $(w_{1,4}, w_{2,4}, w_{3,4}) = (-1,1,1)$ \\
\indent \qquad\qquad The bias: $b_4 = 0$ \\ \\
Next, we substitute these values into the formula for weighted input:
\begin{eqnarray*}
n_4 = \sum_{j=1}^{3}  a_j  w_{j,4} + b_4 
    =  a_1 \cdot w_{1,4} + a_2 \cdot w_{2,4} + a_3 \cdot w_{3,4} + b_4 
    =      (-1)(-1)      +      (-4)(1)      +       (5)(1)      + 0 
    = 2
\end{eqnarray*}
The weighted input to node 4 is $2$, {\bf Answer:} $n_4=2$.
\bigskip

\noindent

\refstepcounter{ActFunctionCounter}\label{actFunctEx1}
\noindent
{\bf \theActFunctionCounter}. Consider the network shown in Fig. \ref{F:simplenet1}. We want to determine the weighted 
input to node 4 shown on the right-hand side of the network, \ie we want the
value of $n_4$. First we identify the values for the activations of the input 
nodes, the weights, and the bias on node 4. \\ \\
\indent \qquad\qquad The activations on the input nodes: $(a_1,a_2,a_3) = 
(-1,-4,5)$ \\
\indent \qquad\qquad The weights: $(w_{1,4}, w_{2,4}, w_{3,4}) = (-1,1,1)$ \\
\indent \qquad\qquad The bias: $b_4 = 0$ \\ \\
Next, we substitute these values into the formula for weighted input:
\begin{eqnarray*}
n_4 = \sum_{j=1}^{3}  a_j  w_{j,4} + b_4 
    =  a_1 \cdot w_{1,4} + a_2 \cdot w_{2,4} + a_3 \cdot w_{3,4} + b_4 
    =      (-1)(-1)      +      (-4)(1)      +       (5)(1)      + 0 
    = 2
\end{eqnarray*}
The weighted input to node 4 is $2$, {\bf Answer:} $n_4=2$.

\bigskip

\stepcounter{ActFunctionCounter}
\noindent

{\bf \theActFunctionCounter.} Suppose we have the same network as in exercise \ref{actFunctEx1}, except 
$b_4 = 1$. What is the weighted input to node 4? {\bf Answer:} $n_4=3$.

\bigskip

\stepcounter{ActFunctionCounter}
\noindent
{\bf \theActFunctionCounter.} Suppose again that we have the same network as in exercise \ref{actFunctEx1} except this time the activations are $(a_1,a_2,a_3) = (0,0,0)$. What is the 
weighted input to node 4? {\bf Answer:} $n_k=0$.

\bigskip

\stepcounter{ActFunctionCounter}
\noindent
{\bf \theActFunctionCounter.} Suppose some node, call it node $k$, has a threshold 
activation function described by $(u,\ell,\theta) = (1,0,0.5)$ (the same as in 
figure \ref{activationFuntctions}). And suppose $n_k = 2$. What is the activation 
of node $k$?  Since $n_k = 2$, and since $2 > 0.5$, the activation takes the 
upper value of $1$. {\bf Answer:} $a_k=1$.

\bigskip

\stepcounter{ActFunctionCounter}
\noindent
{\bf \theActFunctionCounter.} Suppose node $k$ has a linear activation function with
$m=3$ and weighted input $n_k = 2$. What is the activation of node $k$? {\bf 
Answer:} $a_k=6$.

\bigskip

\stepcounter{ActFunctionCounter}
\noindent
{\bf \theActFunctionCounter.} Suppose node $k$ has a linear activation function with
$m=1$ and weighted input $n_k = -0.5$. What is the activation of node $k$? {\bf 
Answer:} $a_k = -0.5$.

\bigskip

% Piecewise linear exercise must be updated. Do not use m!!!  m = 1 in the way it was presented.

\refstepcounter{ActFunctionCounter}\label{actFunctEx2}
\noindent
{\bf \theActFunctionCounter.} Suppose node $k$ has a piecewise linear activation function with $(u,\ell,m) = (1,0,1)$ and weighted input $n_k = 10$. What is the activation of node $k$? {\bf 
Answer:} $a_k = 1$.

\bigskip

\refstepcounter{ActFunctionCounter}
\noindent
{\bf \theActFunctionCounter.} Suppose node $k$ has a ReLU activation function and weighted input $n_k = -10$. What is the activation of node $k$? {\bf 
Answer:} $a_k = 0$.

\bigskip


\refstepcounter{ActFunctionCounter}\label{actFunctEx2}
\noindent
{\bf \theActFunctionCounter.} Suppose node $k$ has a ReLU activation function and weighted input $n_k = 19$. What is the activation of node $k$? {\bf 
Answer:} $a_k = 19$.

\bigskip

\stepcounter{ActFunctionCounter}
\noindent
{\bf \theActFunctionCounter.} Suppose we have the same network as in exercise \ref{actFunctEx2}, except $n_k = .8$. What is the activation of node $k$? {\bf Answer:} $a_k = .8$.

\bigskip

\refstepcounter{ActFunctionCounter}\label{actFunctEx3}
\noindent
{\bf \theActFunctionCounter.} Suppose node $k$ has a sigmoid activation function
with $(u,\ell,m) = (1,0,1)$. This is the sigmoid function shown in Fig. 
\ref{activationFunctions}. Consult that graph. And suppose the weighted input is 
$0.75$ ($n_k = 0.75$). What, approximately, is the activation of node $k$?  
Find $0.75$ on the horizontal axis and find the vertical coordinate of the 
corresponding point on the graph. {\bf Answer:} $n_k \approx 0.9$.

\bigskip

\stepcounter{ActFunctionCounter}
\noindent
{\bf \theActFunctionCounter.} Suppose we have the same network as in exercise \ref{actFunctEx3} except the weighted input is $0$. What is the activation?  {\bf Answer:} $a_k = 0.5$. 

\bigskip

\refstepcounter{ActFunctionCounter}\label{actFunctEx4}
\noindent
{\bf \theActFunctionCounter.} This is a combined exercise that requires you to determine 
the weighted input and activation for a single node. Suppose we have a network with 
two input nodes (labeled 1 and 2) connected to a third node (labeled 3). \\ \\
\indent \qquad\qquad The activations on the input nodes: $(a_1,a_2) = (1,-1)$\\
\indent \qquad\qquad The weights: $(w_{1,3}, w_{2,3}) = (-1,1)$ \\
\indent \qquad\qquad The bias: $b_3 = 1$ \\ \\
And suppose node 3 has a linear activation function with $m=3$. What is the 
activation of node 3?  First, we compute the weighted input to node 3:
\begin{eqnarray*}
n_3 = a_1 \cdot w_{1,3}  + a_2 \cdot w_{2,3} + b_{3} 
    =     (1)(-1)        +     (-1)(1)       + 1     
    = -1 -1 + 1 = -1
\end{eqnarray*}
Next, we compute the activation from the weighted input.
\begin{eqnarray*}
a_3 = m \cdot n_3  = (3)(-1) = -3
\end{eqnarray*}
{\bf Answer:}  $a_3=-3$.

\bigskip

\stepcounter{ActFunctionCounter}
\noindent
{\bf \theActFunctionCounter.} Same as exercise \ref{actFunctEx4} but: \\ \\
\indent \qquad\qquad The activations on the input nodes: $(a_1,a_2) = (-1,1)$\\
\indent \qquad\qquad The weights: $(w_{1,3}, w_{2,3}) = (0,.5)$ \\
\indent \qquad\qquad The bias: $b_3 = 0$ \\ \\
And suppose node 3 has a piecewise linear activation function with $(u,\ell,m) = (1,-1,2)$ What is the 
activation of node 3?  First, we compute the weighted input to node 3:
\begin{eqnarray*}
n_3 =  (-1)(0)        +     (1)(.5)       + 0 = .5
\end{eqnarray*}
Then 
\begin{eqnarray*}
a_3 = m \cdot n_3  = (2)(.5) = 1
\end{eqnarray*}
{\bf Answer:}  $a_3=1$.

\bigskip

\stepcounter{ActFunctionCounter}
\noindent
{\bf \theActFunctionCounter.} Compute activation for node 3: \\ \\
\indent \qquad\qquad The activations on the input nodes: $(a_1,a_2) = (1,1)$\\
\indent \qquad\qquad The weights: $(w_{1,3}, w_{2,3}) = (0,4)$ \\
\indent \qquad\qquad The bias: $b_3 = 1$ \\ \\
And suppose node 3 has a  linear activation function with slope 4.
{\bf Answer:}  $a_3=20$.

\bigskip

\stepcounter{ActFunctionCounter}
\noindent
{\bf \theActFunctionCounter.} Compute activation for node 5: \\ \\
\indent \qquad\qquad The activations on the input nodes: $(a_1,a_2, a_3, a_4) = (1,1,-1,-1)$\\
\indent \qquad\qquad The weights: $(w_{1,5}, w_{2,5}, w_{3,5}, w_{4,5}) = (0,0,1,1)$ \\
\indent \qquad\qquad The bias: $b_5 = 0$ \\ \\
And suppose node 5 has a threshold activation function with $(u,l,\theta) = (1,-1,0)$.
{\bf Answer:}  $a_5=-1$.

\bigskip

\stepcounter{ActFunctionCounter}
\noindent
{\bf \theActFunctionCounter.} Suppose node $k$ has a sigmoid activation function and 
consider three possible values for the weighted input, \ie $n_k=0$, $n_k = 2$, and 
$n_k=2.5$. How can we design the sigmoid activation function so that the 
activation for $n_k=0$ is $0.5$ while the activation for $n_k = 2$ and 
$n_k=2.5$ is far below $1$?  The key idea is to decrease the slope of the 
sigmoid function thereby ``stretching out'' the S shape. First, we let $u=1$ 
and $\ell=0$. This forces the activation to be $0.5$ when $n_k=0$ regardless 
of the slope $m$. Next, we set $m=0.0001$. The activation in response to 
$n_k=2$ is around $0.50020$ and in response to $n_k=2.5$ is around $0.50025$, 
both of which are far below $1$.

\bigskip
