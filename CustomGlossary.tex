\chapter*{Glossary}
\begin{description}

\item[Activation] value associated with a node. Has different interpretations depending on the context. It can, for example, represent the firing rate of a neuron, or the presence of an item in working memory.
\item[Artificial neural network] (Acronym: ANN) a collection of interconnected units which processes information in a brain-like way.
\item[Biological neural network] a set of interconnected neurons in an animal brain.
\item[Computational cognitive neuroscience] the use of neural networks to study psychology and neuroscience simultaneously.
\item[Computational neuroscience] the study of the brain using computer models.
\item[Connectionism] the study of psychological phenomena using artificial neural networks.
\item[Deep network] A neural network with a large number of successive layers of nodes mediating between inputs and outputs. Deep networks are trained using \emph{deep learning} techniques.
\item[Environment] a structure that influences the input nodes of a neural network or is influenced by the output nodes of a network, or both.
\item[Feed-forward network] a network comprised of a sequence layers where all neurons in any layer (besides the last layer) are connected to all neurons in the next layer. Contains no recurrent connections.
\item[Input node] (Synonym: sensor) a node which takes in information from an external environment. 
\item[Machine learning] the use of statistical techniques to produce artificial intelligence. Uses of neural networks as engineering devices are a kind of machine learning.
\item[Node] (Synonyms: unit, artificial neuron) a simulated neuron or neuron-like element in an artificial neural network. 
\item[Output node] (Synonyms: actuator, effector) a node which provides information to an external environment. 
\item[Recurrent network] a network whose nodes are interconnected in such a way that activity can flow in repeating cycles.
\item[Spike] A discrete event which models the action potential for a neuron.
\item[Strength] A value associated with a weight. Has different interpretations depending on the context. It can, for example, represent the efficacy of a synapse, or an association between items in memory.
\item[Supervised learning] a learning rule in which weights are adjusted using an explicit representation of desired outputs.
\item[Synaptic efficacy] The degree to which a pre-synaptic spike increases the probability of a post-snyaptic spike at a synapse.
\item[Topology] the way the nodes and weights of a network are wired together. A network's ``architecture.''
\item[Unsupervised learning] a learning rule in which weights are adjusted without an explicit representation of desired outputs.
\item[Weight] (Synonyms: connection, artificial synapse) a simulated synapse or synapse-like element in a neural network. 
\item[Weighted inputs] (Synonym: net input) the sum of incoming activations to a node times intermediate weights, plus bias. Intuitively, it is the overall level of input a node is receiving.
%\item[Weighted Inputs] (Synonyms: net input)  Dot product of an input vector and a fan-in weight vector, plus a bias term.
\end{description}