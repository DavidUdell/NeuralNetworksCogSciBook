\chapter*{Glossary}
\begin{description}

\item[Action potential] the rapid change in the membrane potential of a neuron, caused by a rapid flow of charged particles or ions across the membrane that occurs during the excitation of that neuron.
\item[Activation] value associated with a node. Has different interpretations depending on the context. It can, for example, represent the firing rate of a neuron, or the presence of an item in working memory.
\item[Activation function] function that converts weighted inputs into an activation in some node updated rules.
\item[Anterograde amnesia] a type of memory loss or amnesia caused by brain damage in the hippocampus, where the ability to create new fact-based or declarative memories is compromised after the injury. 
\item[Artificial neural network] (Acronym: ANN) a collection of interconnected units, which processes information in a brain-like way.
\item[Ataxia] impaired coordination or clumsiness caused by neurological damage in the cerebellum.
\item[Auditory cortex] regions of the temporal lobes of the brain that process sound. 
\item[Axon] the part of the neuron that carries outgoing signals to other neurons.
\item[Basal ganglia] a structure below the surface of the cortex (subcortical) that is involved in voluntary action and reward processing.
\item[Bias] a fixed component of the weighted input to a node's activation. Determines its baseline activation when no inputs are received.
\item[Biological neural network] a set of interconnected neurons in an animal brain.
\item[Brain stem] the lowest part of the brain that connects to the spinal cord and is fundamental for breathing, heart rate, and sleep.
\item[Cerebellum] a structure below the surface of the cortex (subcortical) involved in balance and fine movements, as well as maintaining internal models of the world for motor control.
\item[Cerebral cortex] the outer layer of the brain characterized by its structural folding (gyri and sulci); underlies complex behavior and intelligence in higher animals. 
\item[Clamped node] a node that does not change during updating. Any activation function associated with the node is ignored, and its activation stays the same.
\item[Classification task] a supervised learning task in which each input vector is associated with one or more discrete  categories. An example would be classifying images of faces as male or female. When classification associates each input with one category only, a one-hot encoding is often used on the output layer.
\item[Computational cognitive neuroscience] the use of neural networks to simultaneously model psychological and neural processes.
\item[Computational neuroscience] the study of the brain using computer models.
\item[Connectionism] the study of psychological phenomena using artificial neural networks.
\item[Convolutional layer] a special kind of weight layer where a set of weights (a ``filter'') is passed over a source node layer to produce activations in a target layer, in the sense of being multiplied by each of a moving sequence of subsets of the input activations. This is related to the mathematical operation of convolution.
\item[Cortical blindness] neurological impairment caused by damage to the occipital lobe that results in blindness or inability to see, without any damage to the eyes. 
\item[Deep network] A neural network with a large number of successive layers of nodes mediating between inputs and outputs. Deep networks are trained using \emph{deep learning} techniques.
\item[Distributed representation] a representation scheme where patterns of activation across groups of neurons indicate the presence of an object. 
\item[Dorsal stream] Pathway that extends from the occipital lobe into the parietal lobes, underlying visuospatial processing of visual objects in space. Damage to this pathway can cause impairment in reaching and grasping for objects. 
\item[Environment] a structure that influences the input nodes of a neural network or is influenced by the output nodes of a network, or both.
\item[Excitatory synapses] synapses where an action potential in a presynaptic neuron triggers the release of neurotransmitters that then increase the likelihood of an action potential occurring in the postsynaptic neuron.
\item[Feed-forward network] a network comprised of a sequence of layers where all neurons in any layer (besides the last layer) are connected to all neurons in the next layer. Contains no recurrent connections.
\item[Firing rate] number of spikes (action potentials) a neuron fires per unit time. Usually measured in hertz, that is, number of spikes per second. A higher firing rate corresponds to a more ``active'' neuron. 
\item[Frontal lobe] forward-most lobe of the brain whose many roles include decision-making, action planning, and executive control. Houses many important regions, including prefrontal cortex (PFC), orbitofrontal cortex (OFC), and motor cortex. 
\item[Generalization] the ability of a neural network to perform tasks that were not included in its training dataset. An example would be a network that was trained to identify 10 faces as male, and 10 as female, being able to perform well on (or ``generalize to'') new faces it has not seen before.
\item[Graceful degradation] a property of systems whereby decrease in performance is proportional to damage sustained. The contrast is with brittle systems, in which a small amount of damage can lead to complete failure.
\item[Hemineglect] neurological impairment caused by damage to regions of the parietal lobes characterized by a lack of attending to anything in one half of the visual field.
\item[Hippocampus] a structure below the surface of the cortex (subcortical) that is involved in long-term memory consolidation and spatial maps. Damage to this structure can cause memory loss, or amnesia. 
\item[Inhibitory synapses] synapses where an action potential in a presynaptic neuron triggers the release of neurotransmitters that then decrease the likelihood of an action potential occurring in the postsynaptic neuron.
\item[Input node] (Synonym: sensor) a node that takes in information from an external environment. 
\item[Learning] In a neural network, a process of updating synaptic weights so that the network improves at producing some desired behavior relative to an error function or other objective function.
\item[Linear activation function] a function that is typically just the weighted input, sometimes scaled by a slope parameter. A piecewise linear activation function clips weighted input at  upper and lower bounds. The ReLU function only clips at a lower bound of 0.
\item[Localist representation] a representation scheme where activation of individual neurons indicate the presence of an object. Example: activation of neuron 25 indicates the presence of my grandmother.
\item[Long Term Potentiation] (LTP) a process by which the efficacy of a synapse is increased after repeated use. LTP is part of the basis of the Hebb rule.
\item[Machine learning] the use of statistical techniques to produce artificial intelligence. Uses of neural networks as engineering devices are a kind of machine learning.
\item[Membrane potential] the voltage that results from the difference in the total sum charge of ions on either side of the cell membrane. A cell at rest typically has a resting membrane potential of -70 mV. 
\item[Neurotransmitters] small chemical packages that transmit signals from one neuron to another via synapses. These packages are released when an action potential in a pre-synaptic neuron stimulates their release from vesicles on the axon terminals into the synaptic cleft where they travel to receptors on the dendrites of a post-synaptic neuron. 
\item[Node] (Synonyms: unit, artificial neuron) a simulated neuron or neuron-like element in an artificial neural network. 
\item[Node Layer] A collection of nodes that are treated as a group. For example, in a feed-forward network every node in one layer can be connected to every node in another layer. The activations in a node layer can be represented with an activatoin vector. Without qualification, ``layer'' means node layer.
\item[Occipital lobe] the lobe located in the back of the cortex where visual processing primarily takes place. 
\item[Output node] (Synonyms: actuator, effector) a node that provides information to an external environment. 
\item[Parallel processing] Processing many items at once, concurrently. Contrasted with serial processing, where items are processed one at a time.  Neural networks are known for processing items in parallel, whereas classical computer process items in serial.
\item[Parietal lobe] the lobe located behind the frontal lobe and above the occipital lobe. This part of cortex plays a role in processing spatial information, integrating multisensory information, and is home to the somatosensory cortex, which processes information about touch sensation.
\item[Performance] Consideration of how a network responds to inputs, while keeping its weights fixed.  Often the term is also used to consider how well it is doing relative to an error function or objective function.
\item[Prefrontal cortex] the front-most part of the frontal cortex, which is involved in executive function, decision-making, and planning. It is also thought to have an attractor-based structure that supports the operation of working memory.
\item[Primary motor cortex] strip in the motor cortex that houses a somatotopic map of the body and controls simple movement production. 
\item[Proposition]  (Synonyms: statement, sentence) an expression that can be true or false.
\item[Prosopagnosia] an impairment in recognizing faces that results from damage to particular regions of the ventral stream.
\item[Receptors] binding sites at the ends of dendrite branches of the post-synaptic neuron where neurotransmitters attach.
\item[Recurrent network] a network whose nodes are interconnected in such a way that activity can flow in repeating cycles.
\item[Reinforcement learning] a form of learning in which a system learns to take actions that maximize reward in the long run.   Actions that produce rewards, or action that lead to actions that produce reward, are reinforced in such a way that agents learn to obtain rewards and avoid costly situation.  In humans, associated with circuits in the brain stem and basal ganglia.  Sometimes treated as a third form of learning alongside supervised and unsupervised learning.
\item[ReLU activation function] (``relu'' is short for ``rectified linear unit'') a linear activation function that is clipped at  0. It's activation is 0 for weighted inputs less than or equal to 0, and it is equal to weighted inputs otherwise. It is a popular activation function for deep networks.
\item[Retinotopic map] A topographic map of locations in the retina. Regions of the brain that are retinotopic maps have the property that neurons near one another process information about nearby areas in visual space.
\item[Sigmoid activation function] an activation function that whose value increases  monotoically between a lower and upper bound. As the input goes infinitely far in the positive direction the value converges to the upper bound. As the input goes infinitely far in the negative direction the value converges to the lower bound.
\item[Soma] (Synonym: cell body) the central part of a neuron, which the dendrites and axons connect to.
\item[Somatosensory cortex] front-most region of the parietal lobe that houses a somatotopic map of the body parts and processes tactile information from the body.
\item[Spike] A discrete event that models the action potential for a neuron.
\item[Strength] A value associated with a weight. Has different interpretations depending on the context. It can, for example, represent the efficacy of a synapse, or an association between items in memory.
\item[Supervised learning] a learning rule in which weights are adjusted using an explicit representation of desired outputs.
\item[Synapse] the junction between nerve cells where a information is transferred from one neuron to another.
\item[Synaptic efficacy] The degree to which a pre-synaptic spike increases the probability of a post-snyaptic spike at a synapse.
\item[Temporal lobe] lobe forward of the occipital lobe and below the parietal and frontal lobes. This region is involved primarily in processing semantic information about what things are and factual information, and also houses several important language areas.
\item[Temporal lobe] lobe forward of the occipital lobe and below the parietal and frontal lobes. This region is involved primarily in processing semantic information about what things are and factual information, and also houses several important language areas.
\item[Thalamus] an internal brain structure that relays information from sensory and motor structures to the cortex.
\item[Threshold activation function] a function that has one value for  input at or below a fixed amount (the threshold) and another value for input above the threshold. Usually the value of the function is less below the  threshold than it is above the threshold.
\item[Threshold potential] the membrane potential of the cell above which an action potential is fired. 
\item[Tonotopic map] a topographic map in the auditory cortex that is organized by frequency of sounds. Similar sounds (in terms of frequency) are processed by neurons that are near one another.
\item[Topology] the way the nodes and weights of a network are wired together. A network's ``architecture.''
\item[Unsupervised learning] a learning rule in which weights are adjusted without an explicit representation of desired outputs.
\item[Ventral stream] pathway that extends from the occipital lobe into the temporal lobes, underlying processing of visual object recognition. 
\item[Visual cortex] rear-most region in the occipital lobe involved in visual processing, where primary and secondary visual cortex are housed.
\item[Weight] (Synonyms: connection, artificial synapse) a simulated synapse or synapse-like element in a neural network. 
\item[Weight layer] A set of weights treated as a group. Often they are the collection of weights connecting one node layer to another, which can in turn be represented by a weight matrix.
\item[Weighted Input] (Synonym: net input)  Dot product of an input vector and a fan-in weight vector, plus a bias term. Notated $n_i$ for neuron $i$.
\end{description}