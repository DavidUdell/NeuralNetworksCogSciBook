\chapter{Word Embeddings}\label{ch_word_embeddings}
\chapterauthor{Ellis Cain, Jeff Yoshimi}

% Purpose of this chapter. (1) how to convert linguistic into inputs to a neural network and how to convert outputs back to linguistic data, (2) how to analyze linguistic data using the tools of linear algebra since that broadly fits neural-network approaches to NLP, and (3) a general sense of computational linguistics as it applies to neural network based NLP

% Also see online doc NLP Exercises
% TODO: Go through chapter and add glossary items
% Add something really basic at the front, like with words on the left, and vectors on the right, and like how do we get from one to the other
% Be consistent token vs. word. Not sure which is better.

This chapter elaborates on the concept of a word embedding, which was briefly discussed in section \extref{wrangling}. A word embedding associates each \glossary{token} in a set of tokens with a set of vectors in a way that that captures important characteristics of the set of tokens. Tokens can include words, word-parts, and other character sequences, but we focus on words here and it is standard to call these ``word embeddings'' rather than ``token embeddings.''\footnote{The concept of a ``token'' varies from one context to another. Tokens can also punctuation, emojis, or word-parts and character sequences of various kinds. Sometimes tokens are extracted automatically from a corpus, for example using byte pair encodings.  See \url{https://en.wikipedia.org/wiki/Byte_pair_encoding}.} In this way words can be ``embedded'' in a vector space, that is, associated with lists of numbers that can in turn be processed by a neural network, and associated with points in a space. These techniques have a long history in linguistics and in the study of neural networks, but have become especially prominent in recent years with the advent of large language models like GPT (section \extref{transformers}). Given the emphasis on converting words and other linguistic entities to vectors, this chapter also sheds further light on the concept of feature engineering (section \extref{wrangling}) and ``wrangling'' data so it can be used in neural networks.

After giving some background on linguistics and natural language processing, we build up to the concept of a word embedding in stages. First we discuss document embeddings (associating whole documents with vectors of numbers), which are simpler to understand and serve as a useful basis for understanding word embeddings. Then we discuss word embeddings. Then we discuss the kind of pre-processing and workflow often involved in actually taking a text, creating an embedding, and feeding it to a statistical model like a neural network. Finally we give a brief summary of ways neural networks are used to process linguistic data that has been numerically coded with a text embedding.\footnote{Formally a word embedding is a function from a set of tokens to a set of vectors, and a document embedding is a function from a set of documents to a set of vectors.  We will use ``text embedding'' as a generic way to cover both cases.}

\section{Background and History}

Linguistics is the study of language, which can be organized in terms of scale, going from smallest to largest unit of study: 
\begin{enumerate}
\item Phonetics and phonology: the study of speech sounds.
\item Morphology: the study of words and word forms.
\item Syntax: the study of the structure or grammar, usually at the sentence level.
\item Semantics as the study of meaning, which can be at a variety of levels (words, phrases, sentences).
\item Pragmatics as the study of intentional meaning or implied meaning, for exams implicit maxims and rules of conversations. 
\end{enumerate}


% Maybe more on comp ling
All of these levels have been studied using computational methods (in particular in the area of computational linguistics), and most of them have been studied in relation to neural networks. In some cases features at these levels are used to generate vector representations of linguistic data (for example, word embeddings, the focus of this chapter). In other cases neural networks have been used to analyze structures at these levels. Here is a bit more information on each level and their relevant to neural networks:

%Here is a little more about each of these levels, and how information at these levels is analyzed in vector or tensor representations suitable for a neural network.

For spoken languages, the smallest unit would be the individual speech sounds that are used to create words. These are known as phonemes, such as the [b] in /bat/ or [p] in /pat/ and are often represented using specialized symbols of the International Phonetic Alphabet (IPA\footnote{See \url{https://en.wikipedia.org/wiki/International_Phonetic_Alphabet}}). For applications such as speech recognition or text-to-speech, researchers may need to generate representations of these sounds in vector form. 
This can be done in a very simple way, as in Elman's ``Finding structure in time''~\cite{elman1990finding}, where they manually identified and selected certain phonological features of phonemes and used a binary vector of n-dimensions to represent those phonemes.
It can also be done using spectrograms. A spectrogram is a plot that shows time on the x-axis, frequency on the y-axis, and uses color to indicate the intensity or amplitude at a given frequency. Converting this data into vector representation can capture the frequency content of phonemes.

%% TODO: Check and fix
%One early approach was to simply manually create feature vectors based on linguistic attributes.  For example, in Elman's ``Finding structure in time''~\cite{elman1990finding}, each of 6 words was associated with a 31-bit vector.  Some examples are shown in figure \ref{elmanWordEmbeddings}.
%
%\begin{figure}[h]
%\centering
%\includegraphics[scale=.5]{./images/elmanWordEmbeddings.png}
%\caption[From \cite{elman1990finding}.]{Some of Elman's word embeddings. }
%\label{elmanWordEmbeddings}
%\end{figure}
%
%These early embeddings were hand-crafted based on the scientist's intuitions or using other simple techniques to ensure certain similarity structures. Besides these hand-crafted methods, automated forms of meaning analysis were developed. These automated approaches take documents and use the information in them to produce ``text'' embeddings.
%

The next level up would be morphology, which focuses on the smallest units of meanings at the word level (morphemes). For example, researchers would break ``hunted'' into its comprising parts ``hunt'' and ``-ed''. While morphemes are not often used directly in terms of embeddings, they are used in tokenization, or when you need to identify the constituent parts of words in a text for processing. 
Neural network models of paste tense transformation were big. They could learn standard rules quickly like hunt to hunted but the exceptions take longer, as in humans. %Jeff's part

The order and hierarchical structures of these words in sentences also matter. This is the study of syntax, which includes structures such as SVO ordering of sentences or prepositional phrase attachment (i.e., ``pet the frog [with the feather]'').
Here, automated methods have been briefly used. Treebanks, or syntactically annotated corpora, have been used to train models to annotate Part-of-Speech, syntactic structure, and dependency relationships.
Within the field of neural networks, syntax has been the center of a giant debate. Chomsky has claimed syntax is an innate module in human brains and that it can't be learned in a neural network, given the ``poverty of the stimulus''. Neural networks can learn these syntactic relationships, though they require large training examples and often are not good at dealing with the same range of phenomena as classical tree-structure approaches. The debate remains healthy! \textit{Citations.}

What about the meaning of words, or semantics? There isn't a clear unit of ``meaning'' for words: meaning is more broad and nebulous. The meaning of a word can be abstract or concrete (``justice'' vs ``cup'') and is often context dependent (``bass'' as the fish or instrument).
\textit{Jeff's comments about state space representation.}
%Semantics. This is meanings of words for example ... The classic place that state space representations matter, vector spaces,  See https://mitpress.mit.edu/9780262681575/semantic-cognition/ 
% https://www2.informatik.uni-hamburg.de/WTM/ps/Review%20of%20semantic%20cognition-%20A%20parallel%20distributed%20processing%20approach%20-%20Rogers%20&%20McClelland%202004.pdf
As we will see later in our chapter, modern researchers have developed computer algorithms to generate vector representations of word meanings based on usage patterns of those words.

The last traditional level of linguistics is pragmatics, which can be understood as the implicit rules for language use. It is often not the literal meaning of words (i.e., semantics), but rather the meaning behind the usage. For example, if you asked someone if they studied for the exam, and they said that they ``opened the textbook,'' you likely would understand that they mean that they didn't prepare well for the exam, even though their utterance does not literally mean that. Grice is well known philosopher in this area \cite{grice1957meaning, grice1975logic}, and there has been a recent trend in using Bayesian statistics to model this type of pragmatics through Rational Speech Acts \cite{goodman2016pragmatic}. Pragmatics has not been studied much in neural networks, but has become important with LLMs since they can have actual conversations (See section \extref{transformers}).

 % Do this for each scale. Make sure all terms are defined. For the pragmatic level mention Grice with a citation.


\section{Document embeddings}

% Some history on this and why it was done would help. Why do people do this? How is this useful? 
One simple approach to document embedding is the \glossary{bag of words} approach, which associates documents with vectors of word frequencies. In these representations we don't care about the order in which tokens occurs in a text, hence the term ``bag''. This approach ignores grammatical structure and just looks at how often different tokens occur in different documents.  Simply put, we take each document and put the associated tokens into a bag, and count up how many tokens occur in each bag. Consider the following documents

\begin{description}
\item[Document 1]  ``The bass fish played the bass''
\item[Document 2]  ``The fish played fish with the fish monger''
\end{description}

Each document will be associated with a bag of words, as shown in figure \ref{exampleBags}.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline
    Bag of words & the & bass & fish & played & with & monger \\
    \hline
    Document 1 & 2 & 2 & 1 & 1 & 0 & 0 \\
    Document 2 & 2 & 0 & 3 & 1 & 1 & 1 \\
    \hline
    \end{tabular}
    \caption{Bag of words representation}
    \label{exampleBags}
\end{table}

These document-level bag of words embeddings have a number of problems. First, they ignore the syntactic structure of the sentences in the documents they encode.  Second, they are influenced by the uneven distribution of certain terms in natural languages \cite{piantadosi2014zipf, zipf1945meaning}. To counteract this, we can use TF-IDF (term frequency-inverse document frequency) to calculate the importance of a specific term for a (set of) related document(s), which offsets how frequently a term appears (term frequency) by the number of other documents that also contain the term. This allows it to focus on meaningful terms and not those that generally appear frequently (`the', `a', etc.). 
% Only defined for sets of documents. Places higher weight on terms that occur frequently in one document but not in others. Terms that occur frequently in all documents are given less weight (the inverse document frequency).  Does not address syntax but does deal with distribution issue. Used originally for information retrieval.  This admubrates PPMI in word embeddings.

LSA (latent semantic analysis) is used with a set of documents to analyze semantic information and calculate document similarity. The set of documents is represented using a document-term matrix, where each row corresponds to a document, and each column corresponds to the frequency of a given term (see table \ref{exampleBags}). Then, singular value decomposition (SVD) is used for dimensionality reduction (see \extref{S:dimred}), resulting in a numeric vector for each document (based on term usage/frequency) which can be compared using cosine similarity (section \extref{dotProduct}) to get document similarity.
% So basically SVD on bag of words

% Maybe add some examples. (Ellis: not sure the point of these two sentences.)
The above-mentioned methods are calculated at the document level, in that researchers calculate term frequencies in a given document or set of documents. We can then use these document embeddings to calculate document similarity, as with LSA.

As we will see in the next section, these methods can also be adjusted and applied to the word level as well, allowing us to quantify word meaning as word embeddings. That is, the methods of document embeddings based on occurrence and co-occurrence can be used to come up with word embedding techniques, and these word embeddings can be used to calculate how similar words are to each other.

\section{Word embeddings}

We now move from document embeddings to word embeddings. In this approach, instead of associating documents with vectors, we associate words with vectors. To begin to get a feel what word embeddings are before we discuss the details, see \url{http://vectors.nlpl.eu/explore/embeddings/en/}. 

\subsection{Co-occurrence based embeddings}

% Meaning as use. Wittgenstein.  By connecting usage to meaning, this allows for an approximation or representation of meaning that is derived from usage in a text corpora. 
The high level idea with co-occurrence based embeddings is that two tokens will be close to each other in a vector space if they tend to appear near the same other words in the documents they come from.  Words like ``nurse'' and ``doctor'' will be near each other because they both tend to occur near words like ``hospital'', ``patient'', or ``disease''\footnote{Thanks to Eric Schwitzgebel for this example.}. In other words, because the usage of these words overlap, their meaning should be similar or related to a certain extent.

Theoretical support for this method of computing word embeddings (i.e., tracking co-occurrences allows for a rough estimation of word meaning relative to other words) is provided by the theory of distributional semantics \cite{harris1954distributional, firth1957synopsis} and other usage-based theories of language \cite{wittgenstein1953philosophical}, which posit that the usage of words reflects their meaning, and information about the meaning of words is embedded in linguistic context and the statistical properties of language usage. This is often illustrated with the following quote, attributed to John Firth: ``You shall know a word by the company it keeps.'' For example, when someone talks about a \textit{river}, they may also mention \textit{water} or \textit{bank} (as in river bank), helping the listener correctly decode the intended meaning. Or, from the quote, you are able to interpret \textit{company} as referring to other words in the sentence, and not a \textit{business}, based on the earlier context.

Word embeddings are very important in neural networks and machine learning. A widely-used word embedding is GloVe (Global Vectors for Word Representation, \cite{pennington2014glove}), which tracks the linguistic context and word co-occurrences to ``embed'' words into a semantic space. The general idea, again, is that the generated embedding space is much like our own semantic space \cite{lewis2019distributional}, with the advantage that we can track and compare word meanings mathematically and visually.

Here is an example that illustrates the idea:
\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline
    Words & Dimension 1 & Dimension 2 & Dimension 3 & Dimension 4 & ... & Dimension \textit{n} \\
    \hline
    Book    & 0.8 & 0.1 & -0.2 & 0.5 & ... & 0.7  \\
    Author  & 0.7 & 0.2 & -0.1 & 0.6 & ... & 0.6 \\
    Critic  & 0.6 & 0.3 & -0.1 & 0.4 & ... & 0.5  \\
    \hline
    Painter & 0.2 & 0.8 & 0.4 & 0.1 & ... & -0.3 \\
    Painting& 0.3 & 0.7 & 0.5 & 0.2 & ... & -0.2  \\
    \hline
    Music   & -0.1 & -0.4 & 0.9 & 0.3 & ... & 0.5  \\
    Composer& -0.2 & -0.3 & 0.8 & 0.4 & ... & 0.6 \\
    \hline
    \end{tabular}
    \caption{Vector space embedding representation. Note that between horizontal lines the numbers are closer to each other.}
    \label{exampleEmbeddings}
\end{table}

Note that the columns themselves (the dimensions of the vector space) don't have any clear interpretable meaning\footnote{See the linear algebra chapter. These columns are sometimes called feature but feature implies something that can be easily interpreted.}. Generally speaking, word embeddings are trained on a large text corpus, after which a type of dimensionality reduction is used, usually resulting in a set of 300D vectors. After dimensionality reduction, the columns can no longer be interpreted as co-occurrence with specific terms in the corpus. Rather, it is the pattern across them that matters, as the relative semantic relationships between tokens are encoded in this semantic space. The nice thing, again, is that we can actually picture these by projecting from the high dimensional embedding space to two dimensions and then literally see the distance relationships between tokens, as in figure \ref{f:writerPainterExample}.  

Using pre-trained embeddings, we plotted these seven words to illustrate that the relative relations between tokens are preserved (Fig. \ref{f:writerPainterExample}). Namely, the creator of a medium (i.e., composer, author, painter) is closest to the medium or composition (i.e., music, book, painting). Interestingly, ``critic'' is closest to ``author'', perhaps due to most critics writing their criticism.
% TODO: refs above


\begin{figure}[h]
    \centering
    \includegraphics[scale=.5]{./images/Word_vector_demo.png}
    \caption[Generated using \url{http://vectors.nlpl.eu/explore/embeddings/en/}.]{Example of word embeddings in a semantic space. An embedding space for the seven words: ``book'', ``author'', ``critic'', ``music'', ``composer'', ``painter'', ``painting''. }
 \label{f:writerPainterExample}
\end{figure}

% Todo: no single quotes. Glossary or ital instead? Also a problem is that target and label have other meanings in the book.  In 
% Jeff can do a pass just expanding this a bit, defining each part, maybe with some pictures. How do they work on edges? Or we can work on it  together. One specific question that comes up below is what to do with windows, do they scan across sentences?

%% skip-gram is similar to masked token prediction -- using the bi-directional context, predict the masked target.

\subsection{Co-occurrence Based Word Embeddings}

As we saw in the above section, instead of analyzing large corpora by hand, computational algorithms can be used to generate these numeric representations called word embeddings. Here we give a high level discussion of co-occurrence based word embeddings; an actual example is worked out in section \ref{createWordEmbeddings}. Again, given that the usage of a word captures its meaning, we can track the patterns of how words occur together (i.e., co-occur), which this data is then used as the basis for word embeddings.

These usage patterns are represented in a co-occurrence matrix. To construct a co-occurrence matrix, we must iterate across every sentence in the document or training corpora, where we iterate across each word and count the co-occurrences with the surrounding context. Each word is iterated as the `label' / `target', while the surrounding words serve as the `context'. A window size is defined, which designates how many of the surrounding words to include in the `context'. This context can be either unidirectional, using the preceding text, or bidirectional, using all surrounding context. Once the whole training corpora has been processed, the result is a co-occurrence matrix where each cell represents the raw co-occurrence counts for a given label-context pair. 

% Also good but needs to be broken out a bit more into parts and definitions.  In particular for PPMI
Even at this point, if we compare across the rows of related tokens, their co-occurrences should generally be similar. Still, not every word is used with the same frequency and may negatively impact the quality of our word embeddings. Similar to our document embeddings, some determiners (`the', `a') may be over-represented and skew the co-occurrence matrix. 

Generally, there are two approaches: filtering and normalization. The simplest fix is to simply filter out these high-frequency, low-impact words. These words are called  \emph{stopwords}, and various NLP packages will have stopword lists for various languages.

Another approach (usually in addition to removing stopwords) is to normalize word embeddings so that common words don't drown out the vector representations. A common method is using a positive-pointwise mutual information (PPMI) transform to weight the matrix\footnote{ Lenci~\cite{lenci2018distributional} explains PPMI as measuring ``how much the probability of a target-context pair estimated in the training corpus is higher than the probability we should expect if the target and the context occurred independently of one another.''}.

PPMI weights the co-occurrence values to avoid word-frequency-bias in embeddings. Words like ``the'' and ``a'' that should not be considered meaningful in terms of co-occurrence are down-weighted. Less frequent words on the other, like ``platitude'' or ``espresso'', that are more meaningful in terms of co-occurrences, are up-weighted. The result is a set of \textit{n}-dimensional vectors for a set of words, which are referred to as ``word embeddings.'' For a more in-depth explanation and discussion of word embeddings and distributional semantics, see \cite{lenci2018distributional}.

\subsection{Neural Network Based Embeddings}

Co-occurrence based word embeddings are not the only option. Besides these, neural networks can also be used to create word embeddings. A well-known example is Word2Vec \cite{mikolov2013distributed}, which trains shallow neural networks to predict targets based on the surrounding context\footnote{A code-based walk through of the algorithm is here:  \url{https://www.tensorflow.org/tutorials/word2vec}.}. After the network is trained, the weights of the hidden layer are extracted to serve as the word embeddings. In other words, sometimes co-occurrence based word embeddings are used as data for a neural network, whereas other times neural network based embeddings may be used for a different neural network.

% BERT, discussed in transformer chapter briefly, is also a very complex embedding, that is associated with the rise of transformers.

Another kind of embedding is one that is just filled with random vectors that we then train with gradient descent (see section \extref{sect_gradient_descent}) relative to some kind of task, like next word prediction (see chapter \extref{ch_transformers}), which gradually trains the embedding to work well relative to the required task.

\subsection{Using a word embedding to make a document embedding}\label{wordEmbeddingMatrix}

Of course word embedding can be used to generate document embeddings, simply by associating each token in  a document with its vector embedding and concatenating the result. Thus we can associate a whole document with a matrix, where each row is the vector embedding for one word or token in the document. A prominent example where this idea is used is with LLMs like ChatGPT (see chapter \extref{ch_transformers}), where a context window---a set of prompts and responses ---is converted into a matrix using a vector embedding. Let's take a small, brutish context window:  ``dog chases cat and cat chases dog!''.  First we tokenize the input, then associate each token with an index, like this (notice that the punctuation mark is a token):

\begin{center}
\begin{tabular}{c@{}c@{}c@{}c@{}c@{}c@{}c@{}c}
   3 & 1 & 4 & 5 & 2 & 1 & 3 & \hspace{0.5cm}6 \\
   \texttt{dog} & \texttt{\quad chases} & \texttt{\quad cat} & \texttt{\quad and} & \texttt{\quad cat} & \texttt{\quad chases} & \texttt{\quad dog} & \texttt{\quad !} \\
\end{tabular}
\end{center}

Now we can take each integer and associate it with a row of an embedding matrix, like this, shown here with integer labels on rows to make the idea clear.
\[
\left[\begin{array}{c|ccc}
    1 & 0.5 & 0.1 & 0.3 \\
    2 & 0.2 & 0.4 & 0.6 \\
    3 & 0.7 & 0.8 & 0.9 \\
    4 & 1.0 & 1.1 & 1.2 \\
    5 & 1.3 & 1.4 & 1.5 \\
    6 & 0.3 & 0.1 & 1.2 \\
\end{array}\right]
\]

If we take each token and the corresponding row for it, and stack the results vertically, we end up with a matrix representation of a set of words (that is, a document, or in an llm, a context window), like this: 

\[
\begin{bmatrix}
    0.7 & 0.8 & 0.9 \\
    0.5 & 0.1 & 0.3 \\
    1.0 & 1.1 & 1.2 \\
    1.3 & 1.4 & 1.5 \\
    0.2 & 0.4 & 0.6 \\
    0.5 & 0.1 & 0.3 \\
    0.3 & 0.1 & 1.2 \\
\end{bmatrix}
\]

So there's our vector embedding for the document,  what we can a ``word embedding matrix'' or a ``token embedding matrix''. This matrix is suitable for processing in a neural network, like an LLM. 

You should confirm that the token embedding matrix above makes sense, that each row corresponds to the corresponding token in the sentence ``dog chases cat and cat chases dog!''

\subsection{Evaluation of word embeddings}

% Eric pointed out this is a dense technical discussion that students could potentially skip. We can probably make that clear to students by what we assign and also just do a pas son this later

How do we know that these embeddings accurately capture the meaning of these tokens? How should these embeddings be evaluated? Previous research on word similarity and relatedness has shown that directly asking for relatedness judgements can accurately capture word relations \cite{finkelstein2001placing}.

Therefore, the general method of evaluation is by collecting a set of human judgements, which theoretically serve as the ceiling of model of model performance, though there are recent discussions questioning this \cite{richie2022inter}.
There are a variety of gold standards that have been used across the field, such as WordSim-353 \cite{finkelstein2001placing, agirre2009study}, however, recent models have already reached or surpassed human performance on these tasks.

% Ellis we should discuss this and unpack
Since the models had reached the theoretical performance ceiling based on the gold standards, it was problematic for the field; how would future changes or improvements be evaluated if performance was already at the max?
In response to this, Felix Hill and colleagues \cite{hill2015simlex} set out to create a gold standard that separates similarity from association (previous standards had ignored the distinction) and therefore create a better gold standard that points the way forward for development. 

In brief, association and similarity are two different concepts. Association refers to a relatedness between two concepts, whereas similarity refers almost to synonymy. A major point in their paper \cite{hill2015simlex} is that these two concepts can be separated.
For example, ``car'' and ``tire'' would be considered associated but not similar, while ``glasses'' and ``spectacles'' would be similar.
Interested individuals can read the paper for a more in-depth discussion and model comparison. This issue has become less prominent with the advent of BERT and LLMs.

While different algorithms may improve the model performance and similarity to our own semantic representations, the corpus quality also has an impact on model performance.
Larger training corpora generally improve the quality of the derived embeddings, since the increased amount of data ideally adds more context to be processed. 
GloVe embeddings are trained on various training corpora, varying from 1 billion tokens to 42 billion tokens \cite{pennington2014glove}.

Beside the amount or size of the training corpora, the type of documents is also important; training solely on works of fiction would lead to different embeddings than a model trained on non-fiction, and so on. It is important to consider the meaning you are trying to capture (generalized or specific to a field, such as medical documents).

\section{Workflow: Creating Word Embeddings}\label{createWordEmbeddings}

In practice there are many steps involved in applying these ideas. Here is a sample workflow or pipeline:
\begin{enumerate}
\item Sentence segmentation.
\item Word tokenization
\item Normalization and filtering
\item Creation of word embeddings
\end{enumerate}

To get a sense for how this works, let's apply this pipelines to a sample document:

\begin{quote}
My work is a matter of fundamental sounds, made as fully as possible. Even though they are fundamental, they bring rich aromatic hints of humor. If people want to have headaches among the overtones, let them. And provide their own aspirin. (Adapted from a letter from Samuel Beckett to Alan Schneider, 1957)
\end{quote}

\subsection{Sentence segmentation} 

For sentence segmentation, the paragraph or document is segmented into sentences. This step is particularly important when using text that has been scanned using OCR, where errors might occur. Our sample document would be segmented into four sentences (given that we are segmenting just on periods):

\begin{enumerate}
    \item \textit{My work is a matter of fundamental sounds, made as fully as possible.}
    \item \textit{Even though they are fundamental, they bring rich aromatic hints of humor.}
    \item \textit{If people want to have headaches among the overtones, let them.}
    \item \textit{And provide their own aspirin.}
\end{enumerate}

\subsection{Word tokenization} 

Once the document has been segmented into sentences, a tokenizer is used to split each sentence into the comprising words. This yields a list of lists of tokens, as in
\begin{enumerate}
    \item \textit{my, work, is, a, matter, of, fundamental, sounds, made, as, fully, as, possible}
    \item \textit{even, though, they, are, fundamental, they, bring, rich, aromatic, hints, of, humor}
    \item \textit{if, people, want, to, have, headaches, among, the, overtones, let, them}
    \item \textit{and, provide, their, own, aspirin}
\end{enumerate}

\subsection{Normalization}

Following tokenization, the words/tokens are generally normalized to remove capitalization or certain punctuation marks, such that the words are consistently in the same form. In this step, stopwords can be filtered out. In our case:
\begin{enumerate}
    \item \textit{work, matter, fundamental, sounds}
    \item \textit{fundamental, bring, rich, aromatic, hints, humor}
    \item \textit{people, headaches, overtones}
    \item \textit{provide, aspirin}
\end{enumerate}

\subsection{Create the word embeddings}

For a  basic word embedding algorithm, the label-context pair co-occurrences are tracked in a co-occurrence matrix. For the first sentence, we would start with ``work'' as the first target. Then, given a bidirectional window size of 2, the surrounding context would be ``matter'' and ``fundamental''. Therefore, the co-occurrence pairs would be [work, matter] and [work, fundamental]. We would then create a co-occurrence matrix, with targets on the rows, and context on the columns.  After processing the first sentence, the co-occurrence matrix can be seen in table \ref{exampleCoocmat}:

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    Targets & work & matter & fundamental & sounds \\
    \hline
    work & 0 & 1 & 1 & 0 \\
    \hline
    matter & 1 & 0 & 1 & 1  \\
    \hline
    fundamental & 1 & 1 & 0 & 1  \\
    \hline
    sounds & 0 & 1 & 1 & 0  \\
    \hline
    \end{tabular}
    \caption{Co-occurrence matrix. Each word vector would be a row in the matrix. Note that this is \textit{only} for the first sentence.}
    \label{exampleCoocmat}
\end{table}

Once every sentence has been processed, the final co-occurrence matrix can be seen in figure \ref{coocExample}.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.4]{./images/full_cooc_matrix.png}
    \caption[Generated using Simbrain.]{Co-occurrence matrix for the example sentences. Bidirectional window size of 2, without PPMI.}
 \label{coocExample}
\end{figure}

After processing the whole document and calculating a co-occurrence matrix, we would use PPMI to weight the vectors (recall that PPMI is used to normalize word embeddings so that common words don't drown out the representations). The result can be seen in figure \ref{ppmiExample}.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.4]{./images/weighted_cooc_matrix.png}
    \caption[Generated using Simbrain.]{Resulting co-occurrence matrix after PPMI has been applied. Bidirectional window size of 2.}
 \label{ppmiExample}
\end{figure}