\chapter{New Material for Linear Algebra}
\chapterauthor{Jeff Yoshimi}

% See "by hand" lectures Tom Yeh.
% Nice pics and walk through here: https://e2eml.school/transformers.html
% Outer product as a way of referring to column times row. See ency of scott "inner vs outer product"

\subsection{Elementwise (Hadamard) Product}\label{hadamard}

The elementwise (or Hadamard) product of two vectors or tensors $\mathbf{a}$ and $\mathbf{b}$ with the same shape is denoted by $\mathbf{a} \odot \mathbf{b}$ and is defined as a vector or tensor of the shape whose entries are the products of the corresponding components of $\mathbf{a}$  and $\mathbf{b}$. That is, we multiply corresponding components of the two vectors to produce a new vector with the same shape as the original. In a way this is the most intuitive concept of multiplying vectors or tensors. Just line them up and multiply.

For example
\begin{eqnarray}
(1,2) \odot (3,4) = (1 \cdot 3,  2 \cdot 4) =  (3,8)
\end{eqnarray}

The operation also works on matrices and higher-rank tensors. For example

\begin{eqnarray}
\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix} \odot \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix} &=& \begin{pmatrix} 1 \cdot 5 & 2 \cdot 6 \\ 3 \cdot 7 & 4 \cdot 8 \end{pmatrix} = \begin{pmatrix} 5 & 12 \\ 21 & 32 \end{pmatrix}.
\end{eqnarray}


\subsection{The scalar and vector tracks}\label{scalarVectorTracks}

% TODO: Pictures for source-target representations. This is where to really emphasize that point.

% Compare https://youtu.be/Smav86u60FM?t=390 (from neurons to tokens, from weighted combinations of scalars to weighted combinations of vectors)

%To get to the next belt level, you have to deal with this kind of thing, and so train your brain to deal with it.

The book can be thought of as involving two tracks: a `scalar' and a `vectorized' track. In the scalar track, we think of all operations from the standpoint of individual neurons and their values. We index weights using a source-target or $\textbf{W}_{i,j}$ structure where indices go from $i$ to $j$. In Simbrain, the focus is mostly on separately presented nodes using a ``ball and stick'' representations. We use vectors and matrices to understand many ideas conceptually, but we do not make heavy use of matrix products or matrix algebra. This is much more accessible to beginning students, and is sufficient to teach an entire class. Indeed, this has been the way the book was structured and is the main way Simbrain 3.0 works. This supports the main thrust of the book and Simbrain, making neural networks easy enough to use without any math background at all (there is math to learn, but it is self contained). 
% Source target entails a different representation of matrices

However, this is not how things are done by most professionals working on neural networks in industry, and most academics as well (see ch 2 taxonomy). For any serious work, we must move to the vectorized approach. Everything becomes more concise, and runs more efficiently on high performance performance computers. This requires a change in thinking, a way of seeing and thinking about these problems in terms of matrices, vectors, and tensors, often seeing datasets and batches in new ways, almost as tubes of information that get transformed. The focus here is on vectors, matrices, and higher rank tensors, and on applying linear algebra methods directly to these structures. The weights are now indexed using a target-source scheme, or $\textbf{W}_{j,i}$. In Simbrain 4.0 we have been and are making efforts to make this mode of thinking more intuitive. As of now, there is very little in this format in the book, but we are in the process of adding chapters and sections that present that point of view. Ultimately the hope is to have enough material that a more advanced course could be done entirely from that point of view.

One downside of this is that the indexing changes depending on what you read, but we hope this downside can be an upside. The main book allows a non-math approach, but those going to the target-source point of view must also develop an ability to switch between representations, to see it both ways, which is more complex cognitively but in a way that suits the more advanced mind sets. Many problems are like that, allowing for multiple representations.

% Transposes. Also address in line vectors and whether they need the transpose. There's this weird situation. In simbrain and in general, the default is columns.  But in a book, the default is row. So to show column we do transose operator. But if bold face alone, it is colulmn

\subsection{Graphical conventions}

To think in terms of vectors, we need to bridge between conventional linear algebra notation, and conventional neural network representations, as in Simbrain. We need a template for mentally moving back and forth between these ways of thinking. 

The first thing to do is get clear on the shape of the matrix. Remember, we are using a target-source or $\textbf{W}_{j,i}$ representation. It feels backwards but it's how it's done. So, in figure \ref{linalgToSimbrain}, start on the right with Simbrain. Count the number of outputs, then the number of inputs. Then we have a matrix using the target-source form. So $3$, $2$, and a $3 \times 2$ matrix.

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{images/LinalgToSimbrainReps.png}
\caption[Jeff Yoshimi.]{A set of images to help link standard visual representations in linear algebra (left) with standard visual representations of neural networks (right), via the intermediate representation in the middle. }
\label{linalgToSimbrain}
\end{figure}

Figure \ref{linalgToSimbrain} is an effort to bridge this gap. On the left is a standard matrix product, $\mathbf{W} \mathbf{x} = \mathbf{y}$. This is how forward propagation of an input vector through a weight matrix is usually understood. The weight matrix is $\mathbf{W}$, the input is $\mathbf{x}$, and the output is $\mathbf{y}$. We think of the matrix as operating on the column vector to produce a new column vector.  

\begin{figure}[h]
\centering
\includegraphics[width=0.15\textwidth]{images/simbrain4_ff_2_3.png}
\caption[Jeff Yoshimi.]{Graphical conventions for activation vectors and weight matrices in Simbrain 4. }
\label{simbrain4_ff23}
\end{figure}
% need a transpose section
Here is how I suggest you think about this to connect it up to intuitions about neural networks. Think of the column vector $\mathbf{x}$ as being rotated 90 degrees or transposed to become a row vector (imagine grabbing the vector by the $1$ and pulling it up and to the right). Now mentally take this row and move it to the bottom of the weight matrix, as in the middle panel of  \ref{linalgToSimbrain}. The weight matrix and the output vector stay in the same orientation. The fan-out weight vectors from the input are shown as vertical lines, and the fan-in weight vectors to the output are shown as horizontal lines. Now we can easily ``follow'' the activations upwards as activity is propagated. 
% The bridge is pretty close to the new simbrain rep. Show that?
% Note there are other reps too, like the points in a space rep.

I like to think of the input vector as being dotted with each fan-in weight vector on the outputs to produce the entries of the output vector.

To go from the bridge to the standard Simbrain representation in the right panel we now leave the inputs in place, and imagine the outputs being rotated 90 degrees and moved to the top, and imagine the weights kind of following along. Hopefully you can just see it. 

In Simbrain 4.0, we have designed the representation of activation vectors and weight matrices to follow the ``bridge'' concept above as closely as possible. See figure \ref{simbrain4_ff23}.


%
%Also just a thought on how to think about the weight matrices. Their shape is target-source, or output-input. So if we have a 3-5-2-1 network in terms of node layers, we have three weight matrices. Their shapes are:
%
%5x3
%2x5
%1x2
%
%So you would pass a list of the form [5x3, 2x5, 1x2] to your function. Remember, weight matrices on the left are multiplied by input and activations vectors on the right.I find this pretty counter-intuitive and takes me a lot of practice to think in this “output-input” way. I want to read left-to-right input-to-output but the standard conventions with matrix product reverse this...

\subsection{Matrix Product}

% This is awesome: http://matrixmultiplication.xyz/
% Add a picture showing lines and how they are lined up together. Use the drawings Scott made (see FINAL_ProposalPacket_Preview_IES_Exploration_JeffYoshimi_v1).  


% Connect to row space and column space perspectives.

Consider a matrix multiplication 
\begin{align*}
\mathbf{A}\mathbf{B} = \mathbf{C}
\end{align*}

% Need a picture for this one
This multiplication can be interpreted in several complementary ways, each offering unique insights depending on the context. Perhaps the most common way to learn matrix multiplication is ``entrywise'': the $(i,j)$ entry of the result $\mathbf{C}$ is computed by taking the dot product of the $i$th row of $\mathbf{A}$ and the $j$th column of $\mathbf{B}$:
\[
c_{ij} = \sum_k a_{ik} b_{kj}
\]
See figure \ref{entryWiseMatrixProduct} for an illustration. 

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{images/matrixProductEntryWise.png}
\caption[Jeff Yoshimi.]{Row $i$, column $j$ of $\mathbf{C}$ is the dot product of row $i$ of $\mathbf{A}$ and column $j$ of $\mathbf{B}$. Here entry $(2,1)$ of $\mathbf{C}$ is the dot product of row $2$ of $\mathbf{A}$ and column $1$ of $\mathbf{B}$.}
\label{entryWiseMatrixProduct}
\end{figure}

However, often when dealing with a neural network it is nice to be able to  zoom-out and not worry about specific entries, but just see that rows or columns are being ``processed'' by the matrix. This is a useful conceptual way to understand what is happening. Whether we focus on row processing or column processing is a matter of convention, but I believe the row processing perspective is slightly more common when thinking about convolutional networks or transformers. \footnote{A similar conventional choice occurs when deciding between  the source-target representation of weight matrices (row perspective) and the target-source representation of weight matrices (column perspective), as discussed in note \ref{sourceTarget}.}

Row perspective. One way treats $\mathbf{A}$ as a stack of rows, peels them off, and right multiplies by $\mathbf{B}$ to build up $\mathbf{C}$. Or think of the rows of $\mathbf{A}$ and then linear combinations of the rows of $\mathbf{B}$ to get rows of $\mathbf{C}$. This is common with transformers and batch processing.\footnote{See \url{ https://e2eml.school/transformers.html}.} This approach is useful in considering batch operations and in understanding how representations ``move'' through a transformer.

\begin{align*}
\begin{bmatrix}
\text{--- } \mathbf{a}_1 \text{ ---} \\
\text{--- } \mathbf{a}_2 \text{ ---} \\
\vdots \\
\text{--- } \mathbf{a}_m \text{ ---}
\end{bmatrix}
\cdot
\mathbf{B}
=
\begin{bmatrix}
\text{--- } \mathbf{a}_1 \mathbf{B} \text{--- } \\
\text{--- } \mathbf{a}_2 \mathbf{B} \text{--- } \\
\vdots \\
\text{--- } \mathbf{a}_m \mathbf{B} \text{--- }
\end{bmatrix}
\end{align*}

Column perspective. Think of a matrix multiplication $\mathbf{A}\mathbf{B} = \mathbf{C}$ as taking the columns of $\mathbf{B}$ and then using linear combinations of the columns of $\mathbf{A}$ to get columns of $\mathbf{C}$. One nice way to do this is to actually write $\mathbf{C}$ as a bunch of columns $\mathbf{A}\mathbf{b}_1, \dots, \mathbf{A}\mathbf{b}_n$. \footnote{See \url{https://www.coursera.org/learn/matrix-algebra-determinants-and-eigenvectors/supplement/mGkyi/matrix-operations}.}

\begin{align*}
\mathbf{A} \cdot
\begin{bmatrix}
\vert &        & \vert \\
\mathbf{b}_1 & \cdots & \mathbf{b}_n \\
\vert &        & \vert
\end{bmatrix}
=
\begin{bmatrix}
\vert &        & \vert \\
\mathbf{A} \mathbf{b}_1 & \cdots & \mathbf{A} \mathbf{b}_n \\
\vert &        & \vert
\end{bmatrix}
\end{align*}
